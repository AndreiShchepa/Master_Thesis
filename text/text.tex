
% uncomment the following line to create an unnumbered chapter
\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}\markboth{Introduction}{Introduction}
%---------------------------------------------------------------
\setcounter{page}{1}

Problem, motivation, goal, contributions, and thesis roadmap.
Why smart contract bugs are high impact (immutability, financial incentives) and why existing audits/tools still miss logical flaws. Tell about security audits and how it is time consuming. Then a bit about LLM, which is developed in parallel.
State your contribution precisely: extending Wake-AI with MCP-based RAG + building KB from Solidity vulns/audit knowledge.
One paragraph explaining what each chapter delivers and how it connects. (ethereum, code analysis, large language models, wake framework, proposes methododology, implementation, evaluation, conclusion)

In June 2016, an attacker exploited a reentrancy vulnerability in The DAO smart contract, draining approximately \$60 million worth of Ether in a matter of hours \cite{mehar2019understanding}. The technical flaw was subtle—a recursive call that allowed withdrawals before balance updates—but the consequences were catastrophic. This incident, along with numerous subsequent exploits in decentralized finance (DeFi) protocols, highlights a fundamental challenge in blockchain development: once deployed, smart contracts are immutable, and vulnerabilities cannot be easily patched. Unlike traditional software where bugs might cause inconvenience or data corruption, smart contract vulnerabilities directly translate to financial losses, often measured in millions of dollars \cite{atzei2017survey, perez2021smart}.

The immutability of blockchain systems creates a paradox for security. While immutability is precisely what makes smart contracts trustworthy—users can verify that contract logic won't change arbitrarily—it also means that any security flaw becomes permanent. An attacker who discovers a vulnerability can exploit it with confidence, knowing the code cannot be modified to prevent the attack. Moreover, the financial nature of most smart contract applications creates strong economic incentives for exploitation. When a DeFi protocol manages hundreds of millions in assets, even sophisticated attackers invest significant resources in finding vulnerabilities. This high-stakes environment has driven the development of increasingly rigorous security audit practices, yet critical vulnerabilities continue to slip through.

Current smart contract security relies heavily on manual code audits supplemented by automated analysis tools. Professional audit firms employ expert security researchers who spend weeks examining contract code, looking for common vulnerability patterns, reasoning about edge cases, and attempting to violate protocol invariants \cite{perez2021smart}. Static analysis tools like Slither \cite{feist2019slither} and Mythril \cite{mueller2018mythril} automate detection of known vulnerability patterns such as reentrancy, unchecked external calls, and integer overflows. Fuzz testing tools like Echidna \cite{grieco2020echidna} explore program state spaces to find invariant violations. Despite these sophisticated techniques, significant vulnerabilities continue to evade detection until they are exploited in production.

The fundamental limitation is that many critical vulnerabilities are not simple pattern matches but rather logical flaws that emerge from complex interactions between contract components or violations of implicit business logic assumptions. For example, a flash loan attack might exploit the interaction between a pricing oracle, a lending pool, and a liquidation mechanism in ways that are individually correct but collectively create arbitrage opportunities. Detecting such vulnerabilities requires understanding not just what the code does syntactically, but what it means semantically in the context of the broader protocol design. This type of reasoning has historically required human expertise, making audits time-consuming and expensive—typically taking 2-4 weeks and costing \$50,000-\$200,000 for a medium-sized protocol \cite{consensys2023audit}.

Recent advances in Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and reasoning about code. Models trained on vast corpora of source code, like GPT-4 \cite{openai2023gpt4} and Claude \cite{anthropic2024claude}, can explain code functionality, identify potential bugs, and even generate secure implementations when properly prompted. However, applying LLMs directly to smart contract security faces several challenges. First, LLMs have limited context windows—even models with 200,000 token contexts cannot process entire DeFi protocols with their complex interdependencies and imported libraries. Second, LLMs lack access to the structured analysis results produced by static analysis tools, which provide precise information about control flow, data dependencies, and detected patterns. Third, effective vulnerability analysis requires domain-specific knowledge about Solidity-specific pitfalls, common attack vectors, and audit best practices that may not be well-represented in general code training data.

This thesis addresses these challenges by developing an integrated system that combines the static analysis capabilities of the Wake framework with the reasoning abilities of LLMs through a Model Context Protocol (MCP)-based architecture. Wake is an advanced Python-based framework for Solidity analysis that provides rich programmatic access to abstract syntax trees, control flow graphs, and custom vulnerability detectors \cite{wake2023documentation}. By augmenting Wake with LLM-based analysis through MCP—a protocol that enables structured tool use by language models—we create a system where the LLM can request specific analysis results, retrieve relevant code segments, and access domain knowledge as needed, rather than requiring all context upfront.

% \section*{Contributions}

% This thesis makes the following contributions:

% \textbf{1. MCP-based RAG Architecture for Code Analysis:} We design and implement a Retrieval-Augmented Generation system that enables LLMs to effectively analyze large smart contract codebases despite context window limitations. Our architecture uses the Model Context Protocol to provide the LLM with structured access to code segments, static analysis results, and domain knowledge. Unlike naive approaches that attempt to fit entire contracts into context, our system allows the LLM to iteratively retrieve relevant information based on its analysis needs, enabling it to handle real-world DeFi protocols that span dozens of contracts and tens of thousands of lines of code.

% \textbf{2. Domain-Specific Knowledge Base:} We construct a curated knowledge base of Solidity vulnerability patterns, audit methodologies, and security best practices drawn from public audit reports, vulnerability databases like the Smart Contract Weakness Classification (SWC), and documentation of historical exploits. This knowledge base is structured to support efficient retrieval and provides the LLM with specialized information about blockchain-specific attack vectors that may not be adequately represented in its pre-training data. We develop embedding strategies and retrieval mechanisms specifically tuned for technical security content.

% \textbf{3. Wake Framework Integration:} We extend the Wake-AI project with MCP server implementations that expose Wake's analysis capabilities to LLM agents. This integration allows the language model to request specific analysis operations—such as computing data flow for a variable, identifying all functions that modify a particular state variable, or retrieving functions that match specific structural patterns—and receive structured results that inform its reasoning. The MCP-based design ensures that the integration remains modular and extensible to additional analysis tools.

% \textbf{4. Empirical Evaluation:} We evaluate our system on a dataset of real smart contracts with known vulnerabilities, measuring both detection accuracy and the quality of vulnerability explanations. Our evaluation examines how effectively the RAG-augmented LLM identifies different vulnerability classes compared to baseline static analyzers and explores the system's ability to explain detected issues in ways useful to human auditors. We also analyze failure modes to understand the limitations of the LLM-based approach.

The first chapter provides the necessary background on blockchain technology, the Ethereum platform, and smart contract development. It also establishes the domain context and security challenges that motivate this work.

The second chapter surveys existing approaches to code analysis like static analysis, dynamic testing and formal verification.

The third chapter explores the application of LLMs to code understanding and security analysis. We explain the transformer architecture that underlies modern language models, discuss how these models are adapted for code through pre-training and fine-tuning, and examine their capabilities and limitations for vulnerability detection. 

The remainder of the chapters focus on the design of the methodology proposed that was described above, its implementation and evaluation.

% \textbf{Chapter 5: Retrieval-Augmented Generation and Tool Use} introduces the key techniques that enable LLMs to work effectively with large codebases and specialized tools. We explain Retrieval-Augmented Generation (RAG) architectures, embedding strategies for code, and the Model Context Protocol that allows LLMs to orchestrate external tools. This chapter provides the conceptual framework for our system design.

% \textbf{Chapter 6: Methodology} presents our approach in detail. We describe the architecture of our MCP-based RAG system, the design of our domain knowledge base, the integration with Wake's analysis capabilities, and the retrieval strategies we employ. This chapter explains the key design decisions and trade-offs in building a practical LLM-augmented audit system.

% \textbf{Chapter 7: Implementation} details the concrete realization of our design. We discuss the software architecture, MCP server implementations, database schemas, and integration points between components. This chapter provides sufficient detail for replication and extension of our work.

% \textbf{Chapter 8: Evaluation} presents our experimental results. We describe our dataset of vulnerable contracts, evaluation metrics, baseline comparisons, and analysis of system performance across different vulnerability classes. We examine both quantitative metrics like precision and recall as well as qualitative aspects like explanation quality and practical utility for auditors.

The last chapter summarizes contributions of this work, discusses limitations and results of the proposed approach, and outlines directions for future work.

The goal of this thesis is not to replace human auditors with automated systems—the creative reasoning and contextual understanding required for comprehensive security analysis remains a fundamentally human capability. Rather, we aim to augment auditors with intelligent tools that handle the mechanical aspects of information retrieval and pattern recognition, allowing human expertise to focus on the complex reasoning tasks where it adds the most value. By combining the precision of static analysis, the knowledge retrieval of RAG systems, and the pattern recognition of LLMs, we take a step toward making smart contract security more accessible and effective.

\chapter*{Objectives}
\label{ch:objetives}
Tell a bit about what was done and list objectives that were fullfilled:

\begin{itemize}
    \item Common vulnerability patterns and existing auditing methodologies of Ethereum smart contracts were studied.
    \item A state of the art review on the integration of Large Language Models for automated reasoning and the development of AI-based auditing agents was conducted.
    \item A system that integrates an LLM into the security auditing workflow was designed with further implementation, testing and evaluation.
    \item The obtained results, limitations, and potential improvements of LLM-based auditing systems were discussed in the end.
\end{itemize}


\chapter{Ethereum}
\label{ch:ethereum}
This chapter introduces the foundations needed to understand Ethereum and its components, as this work is closely related to that platform. It first outlines the general principles of blockchain, then specializes in the Ethereum execution model through the Ethereum Virtual Machine (EVM) and the Solidity programming language. Finally, it summarizes common smart-contract vulnerability categories that repeatedly appear in practical audits and will later inform the knowledge base


\section{Overview}
\label{se:overview}
Ethereum, launched in 2015, marked a significant evolution in blockchain technology. First proposed by \texttt{Vitalik Buterin} in a 2013 white paper, Ethereum extends the ledger-centric model to a full application platform that supports smart contracts and decentralized applications (dApps) \cite{Buterin2013}. Rather than simply recording value transfers, it enables general-purpose programmable logic through the EVM while preserving the core properties of transparency, decentralization,and immutability \cite{wood2014yellow}. In effect, Ethereum transformed the blockchain as a generalized state-transition machine, broadening its applicability well beyond simple currency use cases.


\subsection{Blockchain}
\label{sabse:blockchain}
To understand Ethereum’s role as a programmable state-transition system, it is useful to begin with the blockchain foundation on which it operates. A blockchain is a distributed ledger maintained by a network of peer-to-peer nodes rather than a single central authority. It grows as an ordered sequence of blocks, beginning with an initial block (commonly called the \emph{genesis block}), where each subsequent block contains a complete list of transaction records and a cryptographic link to its predecessor. As additional blocks are appended, reversing or altering earlier transactions becomes computationally infeasible. Participants interact through addresses instead of real-world identities, enabling transparency while preserving pseudonymity. Because every node stores a copy of the chain and the full transaction history is publicly verifiable, the system provides strong transparency: value flows can be traced, state transitions inspected, and the integrity of past operations independently confirmed. In short, from the initial genesis block through every appended block, blockchain architecture combines decentralization, tamper-resistance, and verifiable history in a single framework \cite{blockchain}.


\begin{figure}[!h]
    \centering
    \includegraphics[width=10cm, height=9cm]{images/blockchain_diagram_eth.pdf}
    \caption{Structure of an Ethereum blockchain node.}
    \label{fig:blockchain_diagram_eth}
\end{figure}

In Figure \ref{fig:blockchain_diagram_eth}, a blockchain block is shown linking to its predecessor and successor by storing the header hash of the previous block in its own header, forming an immutable chain; the header also includes the Merkle root, a timestamp and a nonce. The lower portion expands the block transaction set into a binary Merkle tree: individual transactions are hashed, then combined pairwise and re-hashed level by level until a single root hash remains. That root inserted in the block header acts as a compact commitment to every transaction in the block, so any alteration to a transaction propagates through the tree and changes the root, invalidating the header. This design allows nodes to verify the inclusion of specific transactions efficiently without reprocessing the entire block, while the previous-hash pointer secures block ordering \cite{blockchain_diagram_eth}.

\subsubsection*{Merkle Tree}
A Merkle tree (hash tree), mentioned in Figure \ref{fig:blockchain_diagram_eth}, is a binary tree in which each leaf holds the cryptographic hash of a data element and each internal node holds the hash of the concatenation of its two children. Starting from a set of items (e.g., transactions), we hash each item to form the leaves, then iteratively hash pairs of nodes until a single value remains: the \emph{Merkle root}. Because each internal label is a hash of its children, any change to any leaf propagates upward and deterministically alters the root. Under standard collision- and preimage-resistance assumptions for the hash function, the root acts as a binding, compact commitment to the entire dataset \cite{merkle1987,nakamoto2008}.

Merkle trees are used in public blockchains to commit to all transactions in a block using only the root in the block header. This design enables efficient \emph{inclusion proofs} (also called \emph{Merkle proofs}) of size $O(\log n)$ for a block with $n$ transactions: to prove that a transaction is part of a block, a verifier only needs the transaction, the header (containing the root), and the minimal set of sibling hashes along the path from the leaf to the root, rather than the entire block. Such proofs underpin ``light'' or SPV clients that validate inclusion without reprocessing all transactions, saving bandwidth and storage while preserving data integrity \cite{nakamoto2008}.


 public blockchain, each n-ode could take part in the consensus process. And onlya selected set of nodes are responsible for validating theblock in consortium blockchain. As for private chain, it isfully controlled by one organization and the organizationcould determine the final consensus mechanism. The two dominant families are proof-of-work (PoW), which selects leaders in proportion to computational power expended, and proof-of-stake (PoS), which selects leaders in proportion to economic stake locked as collateral

\subsection{Consensus mechanisms}
In a PoW system, network participants (miners) compete to solve computational puzzles (e.g., find a nonce so that the hash of the block header meets a target) and thereby propose the next valid block. In contrast, PoS selects block proposers (validators) based on their stake in the network (i.e., the amount of tokens locked up) rather than raw computing power, thereby altering the incentive, security and energy-profile of the system.

\subsubsection*{Proof of Work}
The first concept/method used by blockchains. Generally, the nodes in a PoW-based blockchain network reach consensus by participating in a solution searching process, where each node must find a nonce for its proposed new block. Due to the property of the hash function, the nonce can only be found by repeatedly trying different nonce values until the output is within the target range. When a participant finds the nonce, it will broadcast the block along with the transactions to other nodes. Then, if the new block is verified and determined to be the first block mined after the last block in the chain, it will be integrated into the current chain and become the latest block in the chain. This solution searching procedure can be considered to be a weighted random coin-tossing process where a participant with a higher hash rate(computational power) might have higher chances to be the block winner (leader) who can receive the reward. his computa-tion leads to the large amount of energy consumption forblockchains using PoW consensus mechanisms, as the par-ticipants try to increase their hash rates to have a higherchance to be the leader and receive rewards. Moreover, sinceparticipants with low hash rates have very low chances towin a block and receive rewards, they often join miningpools to have more opportunities to get revenues. A miningpool consists of participants who want to collaborate by con-tributing their computing resources to the pool. In this way,mining tasks will be distributed to the miners, and due to hugecomputing resources, mining pools often get much higher op-portunities to win a new block than individuals. While joininga mining pool provides more stable incomes, the nodes inthe pool often do not contribute to the transaction validationand propagation since they only perform the nonce searchprocess in a specific range. Thus, mining pools have beendominating processes making new blocks in most of currentblockchain networks. For example, the top five mining poolscontrol up to 62.7\% total hash rate of the Bitcoin network [3].This is the most serious issue of PoW-based blockchainnetworks because it is against the decentralized spirit ofblockchain technology. Another issue of PoW protocols isdelay. In a PoW-based blockchain network, when a block isadded to the chain, there is still a possibility that this blockwill not be included in the main chain for several reasons,e.g., network delay causing several versions of the chainor two participants finding two blocks simultaneously. Thispossibility decreases exponentially as the block is deeper inthe chain. Therefore, a block is considered to be finalizedonly when it is a certain k, usually six blocks deep in thechain. This delays the transaction confirmation significantly.Moreover, PoW mechanism is also vulnerable to 51\% attack.In particular, if a single party controls more than 51\% of thenetwork’s total computational power, they can spend theircoins multiple times (in cryptocurrency networks) or preventother transactions by adding conflicting blocks to the chain.While 51\% attacks might not be a serious problem for largeblockchain networks, the newly established networks withsmall and limited total computational power are especiallyvulnerable 


\subsubsection*{Proof of Stake}
The first Proof-of-Stakes (PoS) network, Peercoin [16], wasdeveloped as a PoX consensus mechanism with the aim toreduce the computational requirements of PoW. Participantswith higher coin age, i.e., product of network tokens and theirholding time, have higher chances to be selected. Specifi-cally, each node in Peercoin solves a PoW puzzle with itsown difficulty, which can be reduced by consuming coin age.In the more recent PoS networks, the solution searching iscompletely removed, and the block leaders are no longerselected by computational power. Instead, they are selectedbased on the stakes that they are holding.With the stake-based leader selection process, a node’schance to be selected to be a leader no longer depends on itscomputational power, and thus energy consumption of PoS mechanisms is significantly reduced compared with that ofPoW. Moreover, the block generation and transaction con-firmation speeds are kept at relatively low constant rates bythe PoW networks to ensure security because there are manydifferent blocks proposed by the miners. In contrast, sinceonly one block is made in each round of PoS mechanisms,the block generation and transaction confirmation speedsare usually much faster, and thus PoS mechanism starts tobecome popular recently. 

PoS (Proof of stake) is an energy-saving alternative to PoW.Miners in PoS have to prove the ownership of the amountof currency. It is believed that people with more currencieswould be less likely to attack the network. The selectionbased on account balance is quite unfair because the singlerichest person is bound to be dominant in the network. As aresult, many solutions are proposed with the combination ofthe stake size to decide which one to forge the next block.In particular, Blackcoin [26] uses randomization to predict thenext generator. It uses a formula that looks for the lowesthash value in combination with the size of the stake. Peercoin[21] favors coin age based selection. In Peercoin, older andlarger sets of coins have a greater probability of mining thenext block. Compared to PoW, PoS saves more energy andis more effective. Unfortunately, as the mining cost is nearlyzero, attacks might come as a consequence. Many blockchainsadopt PoW at the beginning and transform to PoS gradually.For instance, ethereum is planing to move from Ethash ( 





With a solid understanding of how blocks are structured and how consensus mechanisms (whether proof-of-work or proof-of-stake) enable nodes to agree upon the canonical chain, we now turn our attention to the next layer of the architecture: how transactions and smart contract code are actually executed

\section{Ethereum Virtual Machine}
The Ethereum Virtual Machine (EVM) is a deterministic state machine that executes the bytecode of the smart contract identically on all nodes \cite{wood2014yellow}. However, it is not Turing complete in the classical sense: execution is strictly resource-bounded by gas - every instruction has a metered gas cost, and execution halts once the supplied gas is exhausted. 
This gas-bounded execution guarantees termination and prevents non-halting or deliberately expensive programs from stalling the network; accordingly, the EVM is often described as \emph{quasi–Turing complete} (computationally universal only under a finite gas budget) \cite{antonopoulos2018mastering}\cite{wood2014yellow}.

\subsection{Gas}
Gas is the unit of account for computational effort in Ethereum. Each EVM opcode has an associated gas cost, and a transaction specifies an upper bound (gas limit) on how much gas may be consumed during its execution.

Gas serves three core purposes: 
\begin{itemize}
    \item \emph{metering} computation and storage, so that the use of the resources is paid for by the initiator of a transaction.
    \item \emph{DoS resistance} and spam prevention, because excessively resource-intensive executions become economically infeasible \cite{wood2014yellow}\cite{article_1559}.
    \item \emph{economic prioritization} of limited block space via fees
\end{itemize}

Since EIP-1559\footnote{\url{https://eips.ethereum.org/EIPS/eip-1559}}, each block includes a dynamically adjusted \emph{base fee} that is burned and a user-specified \emph{priority fee} (tip) paid to the block proposer. 
Users submit \texttt{maxFeePerGas} and \texttt{maxPriorityFeePerGas}; the effective price paid per gas equals \(\min(\texttt{maxFeePerGas}, \text{base fee} + \texttt{maxPriorityFeePerGas})\). 
Blocks have an elastic gas target and the base fee increases or decreases depending on recent block gas usage, stabilizing the fee market while preserving incentives.


\subsection{Architecture}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{images/evm}
    \caption[Ethereum Virtual Machine illustration]{Ethereum Virtual Machine illustration\footnote{\url{https://ethereum.org/developers/docs/evm/}}.}
    \label{fig:evm}
\end{figure}

where 
\begin{itemize}
    \item Program Counter (PC) - component that tracks the current instruction that it is executing;
    \item EVM code - immutable compoennt holds the bytecode of the smart contract that is being executed.
    \item Account Storage - part of the persistent state of the blockchain where the data of smart contracts are stored permanently. There are two types of accounts in the Ethereum environment:\texttt{ (i) }Externally Owned Account (EOA) that is owned by any external entity. Most commonly, it is referred to as a user account; and\texttt{ (ii) }Smart Contract Account that is owned by a smart contract and often controlled by an EOA that interacts with the deployed smart contract \cite{Buterin2013}.
\end{itemize}

\subsection{Memory}

\begin{itemize}
    \item \textbf{Stack} - low-level memory structure that operates on a last-in, first-out (LIFO) principle. It stores small local variables and value types that are required for immediate execution. The stack is limited to 1024 elements, each 256 bits wide, and is accessible only through EVM opcodes or inline assembly. Careful management is essential, as exceeding its capacity causes the execution of the contract to fail \cite{evm_storage}.
    \item \textbf{Memory} - dynamic and expandable storage area allocated for each function execution, which is then discarded once the function has finished. Unlike the stack, which is restricted to temporary data storage, memory allows for more flexible and longer-lived data retention.
    \item \textbf{Storage} - persistent storage is associated with every smart contract and holds its state. Read and write operations in storage are relatively more expensive than those performed in memory. The data in storage are maintained as key-value pairs \cite{evm_storage_arxiv}.
    \item \textbf{Calldata} - read-only and immutable storage that holds function arguments and transaction data for the duration of a transaction call.
\end{itemize}

\section{Smart Contracts}
Building on the blockchain foundation introduced in Section~\ref{se:overview}, Ethereum extends the ledger model with \emph{smart contracts}: programs stored on-chain and executed by all nodes under the same consensus rules \cite{wood2014yellow}. In practice, this means contract execution is deterministic and publicly verifiable: anyone can inspect the code, replay transactions, and validate resulting state changes.

A contract is deployed through a transaction, receives its own on-chain address, and can then be called by externally owned accounts or by other contracts. This execution model reduces reliance on intermediaries and enables composable decentralized applications, where protocols interact as reusable building blocks.

\subsection{Solidity}
Solidity is Ethereum’s dominant high-level, statically typed, contract-oriented language, compiled into EVM bytecode \cite{soliditydocs}. Several language concepts are especially important from a security perspective:

\begin{itemize}
    \item \textbf{Visibility and interfaces} (\texttt{public}, \texttt{external}, \texttt{internal}, \texttt{private}) define which functions are callable and from where.
    \item \textbf{Data locations} (\texttt{storage}, \texttt{memory}, \texttt{calldata}) influence both correctness and gas usage.
    \item \textbf{External call primitives} (\texttt{call}, \texttt{delegatecall}, \texttt{staticcall}) shape trust boundaries and control flow.
    \item \textbf{Failure semantics} (\texttt{require}, \texttt{revert}, \texttt{assert}) determine how safely the contract handles unexpected states.
\end{itemize}

A common drawback of Solidity development is that subtle semantic errors can produce severe vulnerabilities. For example, performing an external call before updating the internal state or careless use of \texttt{delegatecall} may execute foreign code in the caller’s storage context.

\paragraph{Conceptual example}
\begin{minted}[fontsize=\small,breaklines]{solidity}
contract Counter {
    uint256 public value;

    function inc() external {
        value += 1; // persistent on-chain state update
    }
}
\end{minted}

Even this trivial piece of code captures the core model, where a transaction triggers EVM execution, the state is updated, and the transition becomes part of the verifiable blockchain history.

\section{Security}
In Ethereum, security follows directly from the execution model rather than from a separate component: transparent code, immutable state transitions, and adversarially accessible interfaces mean every design decision can become an attack surface. The next section therefore focuses on how vulnerabilities emerge in real Solidity/EVM workflows and how they can be systematically prevented.

\subsection{Vulnerability Taxonomies}
Smart contract security research and industry practice have converged on a set of recurring vulnerability categories. Two widely used resources are the Smart Contract Weakness Classification (SWC) registry and the OWASP Smart Contract Top 10, both of which provide structured names, short descriptions, and representative examples for common classes of issues \cite{swc_registry,owasp_sc_top10}. Using a shared taxonomy is not merely a reporting convenience: it enables consistent labeling across audit reports, static analyzers, and vulnerability datasets, and it makes retrieval-based systems (such as the knowledge base used later in this thesis) easier to query and evaluate.

That said, taxonomies are imperfect abstractions. Many real incidents do not map cleanly to a single label: an exploit may combine oracle manipulation with reentrancy, or rely on an access control mistake that only becomes exploitable due to an accounting bug. For this reason, we treat taxonomies as an organizational tool rather than a complete explanation of risk, and we emphasize concrete vulnerability \emph{patterns} that are actionable during analysis \cite{atzei2017survey,perez2021smart}.

\subsection{Common Vulnerability Types}
% Your “working set” of patterns referenced later by the KB.
\subsubsection*{Reentrancy and External Call Hazards}
\emph{Reentrancy} occurs when a contract makes an external call before it has fully updated its own state, allowing the callee to re-enter and observe or manipulate intermediate state \cite{luu2016making,atzei2017survey}. In practice, the underlying hazard is broader than classic reentrancy: any external call can revert unexpectedly, consume unbounded gas, or execute arbitrary code in the callee, so call ordering and error handling are security-critical \cite{consensys_best_practices}. A common mitigation is the checks-effects-interactions pattern, complemented by explicit reentrancy guards where appropriate \cite{consensys_best_practices}.
\subsubsection*{Access Control and Authorization Bugs}
\emph{Access control} issues arise when privileged operations are callable by unintended parties, or when authorization checks are missing, inconsistent, or based on manipulable state \cite{atzei2017survey,consensys_best_practices}. Typical examples include unprotected administrative functions, incorrect role checks, and unsafe assumptions about \texttt{msg.sender} in proxy and meta-transaction settings. Because Solidity and the EVM do not provide built-in role management, secure authorization is largely a matter of correct application logic and disciplined use of vetted libraries \cite{consensys_best_practices}.
\subsubsection*{DoS and Gas Griefing}
\emph{Denial of service} in Ethereum often takes the form of making a critical operation prohibitively expensive or consistently reverting. Examples include iterating over unbounded lists, relying on external calls that can be forced to revert, or designs where a single malicious participant can block progress by refusing to cooperate \cite{atzei2017survey}. Because gas imposes a hard resource budget, some failures are not ``bugs'' in the conventional sense but brittleness in protocol design under adversarial conditions. Defensive patterns include bounded iteration, pull-based payments, and explicit fail-open/fail-closed decisions with documented assumptions \cite{consensys_best_practices}.
\subsubsection*{Front-running / MEV-related Issues (when relevant)}
Ethereum transactions are visible in the mempool before inclusion, and block producers can influence ordering within a block. This creates opportunities for \emph{front-running} and broader Maximal Extractable Value (MEV) strategies, where adversaries profit from reordering or inserting transactions around a victim transaction \cite{daian2020flash}. In protocol design, this shows up as price manipulation during trades, sandwich attacks, and liquidation races. Mitigations depend heavily on the application and often require mechanism design choices (e.g., commit-reveal, batch auctions, or oracle design) rather than purely local code changes \cite{daian2020flash}.

% For each: 2–5 sentences + 1 tiny example later in Implementation/Eval if needed.

% \section{Audit Methodology}

% Smart contract auditing has evolved into a specialized discipline that combines elements of traditional software security review with domain-specific knowledge about blockchain systems and economic attack vectors. Unlike conventional software where bugs might cause crashes or data corruption, smart contract vulnerabilities can lead directly to financial losses, often measured in millions of dollars TODO(\cite{some_link/footnote}).

% Auditors begin by establishing what the system is supposed to do and identifying potential threat vectors, then systematically examine the implementation through a combination of manual review and automated tools. While human expertise remains central to catching subtle logic bugs and business logic flaws, automation has become increasingly important for handling the scale and complexity of modern DeFi protocols \cite{perez2021smart}. 

% \subsubsection*{System overview}

% \subsubsection*{Trust model}

% Before examining a single line of code, effective audits begin with understanding what the system is designed to do and what could go wrong. This specification and threat modeling phase establishes the foundation for all subsequent analysis. Auditors work with the development team to document the intended behavior of the protocol, including state invariants that should always hold, access control assumptions about who can perform which actions, and economic mechanisms that should remain balanced under all circumstances \cite{groce2018echidna}.

% State invariants are particularly critical in DeFi protocols. For example, an automated market maker (AMM) might have an invariant that the product of token reserves remains constant except during fee accrual, or a lending protocol might require that total collateral always exceeds total debt by some minimum margin. Documenting these invariants explicitly serves multiple purposes: it guides manual review by clarifying what properties to verify, it provides targets for property-based fuzzing tools, and it establishes success criteria for testing. When invariants are violated, it often indicates either a vulnerability in the code or a gap in the original specification that needs addressing.

% Trust assumptions define the threat model—which actors are assumed to be honest, which might be malicious, and what capabilities each has. A typical DeFi protocol might trust its governance multi-sig to upgrade contracts responsibly while assuming that all regular users will act to maximize their own economic benefit, including front-running transactions and exploiting any profitable vulnerabilities. External dependencies also factor into the trust model: does the protocol rely on specific oracle providers, and what happens if those oracles are compromised or manipulated? These assumptions directly influence what the auditor looks for in the code.

% This phase also identifies high-value targets—the functions and state variables most critical to protocol security and most likely to be attacked. In a lending protocol, this might be functions that calculate collateralization ratios or trigger liquidations. In a bridge, it could be the validation logic for cross-chain messages. By explicitly mapping out these critical components and their security requirements up front, auditors can allocate their limited time more effectively and ensure comprehensive coverage of the attack surface. This specification work also sets up what an LLM-based agent should retrieve when analyzing code: knowing the declared invariants and trust assumptions allows the agent to focus its limited context window on retrieving code segments most relevant to verifying those properties.

% \subsubsection{Manual Code Review}

% Manual code review remains the cornerstone of smart contract auditing despite advances in automation. Experienced auditors develop intuition about common vulnerability patterns, but more importantly, they can reason about complex interactions, business logic edge cases, and economic attack vectors that current tools struggle to identify. The review process typically proceeds through several phases, each with a different focus and level of detail.

% The review begins with architectural analysis—understanding how contracts interact, what external systems they depend on, and how data flows through the protocol. Auditors construct mental models (and often diagrams) of the system's components and their relationships. For example, in a yield aggregation protocol, they map out how user deposits flow into various strategies, how yield is calculated and distributed, and what happens during rebalancing or emergency withdrawals. This high-level understanding helps identify architectural vulnerabilities like dangerous centralization points, circular dependencies that could create reentrancy opportunities, or reliance on external systems that could fail.

% With the architecture clear, auditors move to examining critical execution paths. They trace through key user interactions step by step: what happens when Alice deposits tokens, when Bob triggers a liquidation, when governance proposes a parameter change? This path analysis helps identify missing checks, incorrect sequencing, or edge cases where the code behaves unexpectedly. Auditors pay special attention to paths involving external calls, token transfers, and state changes—the operations most likely to introduce vulnerabilities.

% Invariant verification forms another crucial part of manual review. Armed with the invariants documented during threat modeling, auditors examine whether the code actually maintains these properties under all possible executions. This often involves reasoning about potential sequences of function calls and state transitions. For instance, if a protocol claims that "users can always withdraw their proportional share of pooled assets," the auditor checks whether there are any code paths where this could fail—perhaps during contract upgrades, when certain functions are paused, or in response to specific market conditions.

% Edge case analysis requires creativity and experience. Auditors consider boundary conditions: what happens with zero values, maximum values, or just-below-overflow values? They think about timing issues: what if a transaction is front-run, what if the block timestamp is manipulated slightly, what if multiple operations happen in the same block? They examine error handling: are all failure modes properly addressed, or are there paths where errors could leave the system in an inconsistent state? This type of reasoning is difficult to automate because it requires understanding both the code's literal behavior and the protocol's semantic intent.

% The manual review process is inherently limited by human cognitive constraints. An auditor can only hold so much context in working memory at once, and complex protocols might span dozens of contracts with thousands of lines of code. This is where tool-assisted workflows become valuable—not to replace human judgment, but to help auditors manage complexity by automating context retrieval, flagging potential issues for human investigation, and maintaining consistency in coverage across a large codebase. The goal of integrating LLMs into this workflow is to augment human auditors with better information retrieval and pattern recognition, allowing them to focus their expertise on the creative reasoning tasks that machines still cannot perform.

% \subsubsection*{Static analysis}

% \subsubsection*{Local deployment}

% \subsubsection*{Tool-Based Analysis}

% Automated analysis tools have become an essential part of the smart contract audit workflow, complementing manual review by efficiently checking for known vulnerability patterns and coding errors. These tools vary in their approaches—from lightweight pattern matching to sophisticated symbolic execution—and each brings different strengths and limitations to the audit process.

% Static analysis tools examine code without executing it, using techniques ranging from simple syntactic pattern matching to complex abstract interpretation. Slither, developed by Trail of Bits, has emerged as one of the most widely adopted static analyzers for Solidity \cite{feist2019slither}. It operates on Solidity's intermediate representation (IR) and includes over 70 built-in detectors for common issues like reentrancy vulnerabilities, unprotected ether withdrawals, incorrect ERC-20 implementations, and dangerous delegatecalls. Slither's strength lies in its speed and comprehensive coverage—it can analyze a medium-sized project in seconds and rarely produces false negatives for the patterns it targets.

% However, static analyzers face fundamental limitations. They must over-approximate program behavior to be sound, which often leads to false positives—flagging code as potentially vulnerable when it's actually safe. For example, Slither might flag a reentrancy warning for any external call followed by a state change, even when the specific call cannot actually re-enter the contract. Auditors must manually review these warnings to separate real issues from noise. More critically, pattern-based detectors can only find vulnerabilities they were explicitly programmed to recognize. Novel vulnerability classes or complex logic bugs that don't match known patterns slip through entirely.

% Other static analysis tools take different approaches. Mythril employs symbolic execution and SMT solving to explore possible execution paths and find states that violate security properties \cite{mueller2018mythril}. This can uncover deeper issues than pattern matching, but symbolic execution doesn't scale well to large contracts or complex path conditions—the number of possible paths grows exponentially with program size and branching. Securify uses a dataflow analysis framework and security patterns specified in a domain-specific language \cite{tsankov2018securify}, offering more flexibility in specifying what to look for but requiring expertise to write effective patterns.

% Linters like Solhint and Ethlint catch style violations and potential code quality issues rather than security vulnerabilities per se. While less critical than security-focused analyzers, they help maintain code quality that makes contracts easier to audit. They flag issues like missing visibility specifiers, unused variables, overly complex functions, and deviations from best practice patterns like checks-effects-interactions.

% Integration of multiple tools provides better coverage than relying on any single analyzer. A typical audit workflow might run Slither for quick pattern detection, Mythril for deeper analysis of critical functions, and various linters for code quality checks. However, this multi-tool approach creates new challenges: each tool has its own output format, overlapping detections need deduplication, and the auditor must triage dozens or hundreds of findings to identify which require attention.

% This is where the Wake framework becomes relevant to our work. Wake provides a Python-based infrastructure for building custom detectors and integrating various analysis tools in a unified workflow \cite{wake2023documentation}. Unlike standalone tools that operate in isolation, Wake allows auditors to write detectors that combine insights from multiple sources—for example, using both AST pattern matching and data flow analysis to reduce false positives. Wake's Python ecosystem also makes it a natural bridge to LLM-based analysis: we can extract rich context from Wake's analysis passes and feed it to language models that can reason about patterns too complex for rule-based systems.

% The key insight for our work is that automated tools excel at scalable, repeatable pattern matching but struggle with novel vulnerabilities and contextual reasoning. Meanwhile, LLMs demonstrate strong pattern recognition on code they've seen during training and can generate natural language explanations, but they lack the precision and soundness guarantees of formal methods. By combining static analysis tools with LLM reasoning in an agent-based architecture—using tools like Wake to extract structured information and RAG to provide relevant context—we aim to get closer to the best of both approaches.

% \subsubsection*{Local deployment}

% Testing smart contracts in realistic conditions requires deploying them to a local blockchain environment where their behavior can be observed without risking real funds or incurring transaction costs. Modern development frameworks like Hardhat, Foundry, and Wake provide robust local testing infrastructure that has become standard in the audit workflow \cite{hardhat2023, foundry2023}.

% Unit tests verify individual functions in isolation, checking that they produce expected outputs for given inputs and properly handle error conditions. Integration tests examine how multiple contracts interact, ensuring that complex workflows execute correctly end-to-end. For example, testing a lending protocol might involve simulating a sequence of deposits, borrows, interest accrual, and liquidations to verify that all state transitions occur correctly and invariants are maintained. Well-written test suites serve both as verification of correct behavior and as documentation of intended functionality.

% Fork testing represents a particularly powerful technique for smart contract auditing. Rather than testing in isolation, fork testing deploys the contract under audit to a local blockchain that has copied the state of mainnet at a specific block. This allows the contract to interact with real deployed protocols—actual DEXes, oracles, tokens—without requiring the auditor to mock these complex systems. For example, when auditing a yield optimization strategy, fork testing can verify that the strategy actually generates yield when deployed against real DeFi protocols, that it handles real market conditions correctly, and that it responds appropriately to real oracle updates.

% Fork testing also enables reproduction and investigation of historical exploits. By forking at a block just before a known attack, auditors can replay the attack transaction to understand exactly how it worked, then test whether proposed fixes would have prevented it. This forensic capability helps validate that audit recommendations actually address the vulnerabilities they target. Wake's testing framework includes sophisticated fork testing capabilities, allowing tests to manipulate fork state, impersonate arbitrary accounts, and observe detailed execution traces.

% Despite their value, tests have inherent limitations. Test coverage is only as good as the test cases written—they can demonstrate the presence of bugs but never prove their absence. Auditors who rely too heavily on "the tests pass" as evidence of security may miss edge cases that weren't considered when writing tests. Additionally, tests typically focus on expected usage patterns, while attackers deliberately seek unexpected input combinations and state transitions that developers didn't anticipate.

% This limitation motivates property-based testing approaches, where instead of writing individual test cases, auditors specify properties that should always hold and let the testing framework automatically generate inputs trying to violate those properties. This bridges to our next topic: fuzz testing, which takes this idea even further by using randomization and evolutionary algorithms to explore the state space more thoroughly than human-written tests ever could.

% \subsubsection*{Fuzz testing}

% Fuzzing has emerged as one of the most effective techniques for uncovering subtle bugs in smart contracts, particularly logic errors that evade static analysis and aren't covered by manual test cases. Unlike traditional testing where humans specify input values, fuzzing automatically generates large numbers of random or semi-random inputs to explore program behavior across a wide range of scenarios \cite{trail2023fuzz}.

% The simplest form of fuzzing, random input generation, feeds functions with arbitrary values and checks whether they crash, revert unexpectedly, or violate assertions. While naive, this approach can quickly find boundary condition bugs—what happens with zero values, maximum uint256 values, or unexpected combinations? More sophisticated fuzzers use coverage-guided generation, tracking which code paths have been executed and favoring inputs that reach new branches. This evolutionary approach efficiently explores the state space, focusing computational effort on finding inputs that trigger novel behaviors.

% Property-based fuzzing, exemplified by tools like Echidna \cite{grieco2020echidna} and Foundry's fuzzer \cite{foundry2023}, takes this further by allowing auditors to specify invariants that should always hold. The fuzzer then attempts to find any sequence of function calls and inputs that violates these invariants. For example, an auditor might specify that "total shares times share price should equal total assets" for a vault contract. The fuzzer will generate thousands of random deposit, withdraw, and transfer operations, checking after each whether this invariant still holds. When it finds a violation, it automatically minimizes the test case to the simplest sequence that reproduces the issue.

% Stateful fuzzing considers sequences of operations rather than isolated function calls. Many vulnerabilities only emerge through specific state transitions—for example, a reentrancy bug that requires calling function A to set up state, then reentering during a call to function B to exploit that state. Echidna maintains a model of contract state and generates sequences of transactions that explore different state transition paths. This catches bugs that unit tests miss because the test writer didn't consider that particular sequence of operations.

% Fuzzing excels at finding logic bugs—violations of business rules or implicit assumptions that static analyzers can't detect because they aren't simple pattern matches. For instance, fuzzing might discover that a specific sequence of deposits and withdrawals allows a user to extract more funds than they deposited, or that under certain market conditions a pricing oracle can be manipulated. These are precisely the high-impact bugs that make auditing challenging: they're correct in terms of type safety and basic invariants, but incorrect in terms of the protocol's economic logic.

% However, fuzzing has limitations. It's probabilistic rather than exhaustive—finding a bug depends on the fuzzer happening to generate the right sequence of inputs, which might be a tiny fraction of the total input space. Complex bugs requiring specific state setups might be missed if the fuzzer doesn't happen to explore that path. Additionally, writing good invariant specifications requires expertise—if the auditor doesn't correctly specify what should always be true, the fuzzer won't detect violations.

% The integration of fuzzing into the audit workflow has become standard practice. Auditors typically run fuzzers overnight or over weekends, generating millions of test cases to explore state spaces too large for manual testing. When combined with static analysis (which finds known patterns quickly) and manual review (which applies human reasoning about business logic), fuzzing provides a powerful third pillar in the defense against vulnerabilities. For our work with LLM-based agents, fuzzing results provide valuable signal: when a fuzzer finds an invariant violation, the LLM can analyze the failing sequence to explain why it violates the invariant and suggest fixes.

% Rather than viewing automation as competing with human auditors, we see it as extending their capabilities. The techniques we've discussed generate vast amounts of information: static analyzers flag hundreds of potential issues, tests exercise thousands of state transitions, fuzzers generate millions of inputs. Making effective use of this information requires systems that can retrieve relevant context, synthesize insights across tools, and present findings in ways that augment human reasoning.

% The code analysis approaches we examine next build on the audit methodology foundations established here. We explore how static analysis frameworks like Wake provide structured access to code properties, how retrieval systems can surface relevant code segments for LLM analysis, and how agent architectures can orchestrate multiple tools to tackle complex analysis tasks. The goal is not to replace the auditor workflows described in this chapter, but to make them more efficient and effective by automating the retrieval and synthesis steps that currently consume significant auditor time.

\chapter{Code Analysis}
\label{ch:code-analysis}

% Goal: describe analysis methods you'll reference when positioning Wake/Wake-AI and your RAG design.

The security analysis of software systems, particularly smart contracts deployed on blockchain platforms, relies on a diverse set of analytical techniques that have evolved over decades of programming language research. This chapter provides a comprehensive overview of the foundational methods for code analysis, establishing the theoretical and practical groundwork necessary for understanding how modern tools—including Wake and its AI-augmented extensions—approach vulnerability detection in Solidity smart contracts. We organise our discussion around three complementary paradigms: static analysis, which reasons about program behaviour without execution; dynamic analysis, which observes actual runtime behaviour; and formal verification, which provides mathematical guarantees about program correctness.

\section{Static Analysis}
\label{sec:static-analysis}

Static analysis encompasses techniques that examine source code, bytecode, or other program representations to infer properties about all possible executions without actually running the program~\cite{nielson1999principles}. The appeal of static analysis lies in its soundness guarantees—a sound static analyser will never miss a bug of the class it is designed to detect—and its ability to reason about infinite execution paths. However, this power comes at the cost of precision: static analysers may report false positives, flagging code as potentially buggy when no actual vulnerability exists~\cite{rice1953classes}.


\subsection{Abstract Syntax Tree}
\label{subsec:ast}

The abstract syntax tree (AST) serves as the foundational representation for most static analysis techniques. An AST is a hierarchical tree structure that captures the syntactic structure of source code while abstracting away concrete details such as whitespace, comments, and parentheses that do not affect program semantics~\cite{aho2006compilers}. Each node in the tree corresponds to a syntactic construct—a function declaration, an assignment statement, a binary expression—and the tree's structure reflects the nesting and composition of these constructs.

[TODO describe AST maps to code]

However, the AST has significant limitations for semantic analysis. Because it captures only syntactic structure, it cannot directly represent the flow of data through a program or the possible paths of control flow. Two syntactically similar code fragments may exhibit vastly different runtime behaviours depending on the values of variables and the paths taken through conditional statements. These limitations motivate the use of more semantically rich representations.

\subsection{Intermediate Representation}
\label{subsec:ir}

Intermediate representations (IRs) bridge the gap between source-level syntax and low-level execution semantics by normalizing diverse syntactic constructs into a uniform format amenable to analysis~\cite{muchnick1997advanced}. A well-designed IR strips away syntactic sugar, resolves implicit operations, and exposes the underlying computational structure of the program. {FIX OLD CITE}

[TODO smth interesting in general related to code]

\subsection{Control Flow Analysis}
\label{subsec:control-flow}

Control flow analysis (CFA) determines the possible paths of execution through a program, representing this information as a control flow graph (CFG) where nodes represent basic blocks—sequences of instructions with no internal branching—and edges represent possible transfers of control~\cite{allen1970control}. The CFG provides the structural backbone for many subsequent analyses, enabling reasoning about path reachability, loop structure, and the ordering of operations.

[Again old cite, if you cite ,you can directly copy text from book or web page]


The call graph, a related structure that captures the calling relationships between functions, extends control flow analysis to the interprocedural level. Each node in the call graph represents a function, and edges represent potential call relationships. In the presence of dynamic dispatch—common in object-oriented languages and in Solidity's inheritance system—call graph construction becomes challenging, as the target of a call may depend on runtime type information~\cite{grove2001framework}.
entially leading to precision loss.

[TODO some grapth with explanation, image]

Reachability analysis over the CFG determines which program points can potentially be reached from a given starting point. This analysis is fundamental for pruning infeasible paths from consideration and for establishing preconditions under which vulnerabilities can be triggered. If a vulnerable code pattern exists only on unreachable paths, it poses no actual security risk.

\subsection{Data Flow Analysis}
\label{subsec:data-flow}

Data flow analysis tracks how values flow through a program, determining at each program point which values may be produced and consumed~\cite{kildall1973unified}. The classic formulation employs the monotone framework, which computes fixed points over the lattice of possible data flow facts by iteratively propagating information along CFG edges until no further changes occur~\cite{kam1977monotone}.

The reaching definitions analysis, a foundational data flow analysis, determines for each use of a variable which definitions (assignments) might reach that use. This information enables detection of uninitialised variables, dead code, and various forms of data dependency. For smart contract security, reaching definitions help establish whether user-controlled input can influence security-critical operations~\cite{yamaguchi2014modeling}.

Taint analysis, a specialisation of data flow analysis, tracks the flow of untrusted data from sources (such as transaction parameters) to security-sensitive sinks (such as external calls or state modifications)~\cite{schwartz2010all}. In the smart contract domain, taint sources include \texttt{msg.sender}, \texttt{msg.value}, \texttt{calldata}, and return values from external calls. Sinks include operations that transfer value, modify access control state, or perform external calls. A taint flow from source to sink without proper sanitisation indicates a potential vulnerability.

Data flow analysis faces particular challenges in Solidity due to storage aliasing. Solidity's storage model maps state variables to 256-bit storage slots according to complex packing rules, and dynamic data structures such as mappings and arrays use hash-based slot computation~\cite{antonopoulos2018mastering}. Two syntactically distinct storage references may alias to the same slot, creating data flow paths invisible to na\"ive analysis. Precise handling of storage aliasing requires modelling Solidity's storage layout rules and potentially employing alias analysis techniques~\cite{andersen1994program}.

\section{Dynamic Analysis}
\label{sec:dynamic-analysis}

While static analysis reasons about all possible executions, dynamic analysis observes actual program behaviour during concrete executions. This complementary approach sacrifices soundness—a dynamic analysis can only observe the executions it actually runs—in exchange for precision: the behaviours observed during dynamic analysis represent actual, not merely potential, program behaviour. [TODO cite]

\subsection{Fuzz Testing}
\label{subsec:fuzz-testing}

Fuzz testing (fuzzing) is a dynamic testing technique that automatically generates test inputs to explore program behaviour and discover bugs [TODO cite smth normal related to text and fuzzing, not book from 1980 year]. Modern fuzzing has evolved significantly from its origins in random input generation, incorporating coverage guidance, constraint solving, and domain-specific knowledge to systematically explore program state space~\cite{manes2019art}.

[TODO maybe a table with some stats interesting, then refernece and small discussion about it]

Invariant-based fuzzing provides properties that should hold across all reachable states. The fuzzer then attempts to generate transaction sequences that violate these invariants. Common invariants include balance conservation (total value in should equal total value out), access control consistency (only authorised addresses can perform privileged operations), and state machine integrity (the contract should only be in valid states)~\cite{wustholz2020harvey}.

Coverage-guided fuzzing uses feedback from previous executions to guide input generation toward unexplored program regions. By instrumenting the contract to track which code paths are exercised, the fuzzer can prioritise inputs that reach new coverage, systematically expanding the explored state space~\cite{nguyen2020sfuzz}. The combination of coverage guidance with property-based testing provides a powerful technique for discovering subtle vulnerabilities that escape both manual review and pattern-based static analysis.

Corpus generation and management play crucial roles in fuzzing effectiveness. The corpus—the set of seed inputs from which the fuzzer generates new inputs—significantly influences exploration efficiency. Techniques such as corpus distillation, which removes redundant inputs that do not contribute new coverage, and corpus scheduling, which prioritises inputs likely to lead to new discoveries, improve fuzzing throughput~\cite{rebert2014optimizing}.

\section{Formal Verification}
\label{sec:formal-verification}

Formal verification employs mathematical techniques to prove that programs satisfy specified properties with certainty, not merely with high probability as testing provides~\cite{clarke2018model}. While static analysis and testing may miss vulnerabilities or report false positives, formal verification can provide guarantees: if verification succeeds, the property provably holds; if it fails, a concrete counterexample demonstrates the violation.

However, formal verification faces significant practical challenges. The techniques are computationally expensive, often requiring substantial manual effort to specify properties and guide the verification process. Scalability limitations restrict application to smaller codebases or require abstraction that may introduce imprecision. Additionally, verification establishes conformance to specification, but specifications themselves may be incomplete or incorrect, leaving gaps in security coverage.

[TODO mention, that even itis well known existed technique for dynami analysis, it has very weak application in Ehereum and smart contracts]

\subsection{Symbolic Execution}
\label{subsec:symbolic-execution}

Symbolic execution executes programs with symbolic rather than concrete values, representing program state as logical constraints over symbolic variables~\cite{king1976symbolic}. Rather than computing specific outputs for specific inputs, symbolic execution computes expressions over symbolic inputs that characterise the relationship between inputs and outputs. At branch points, execution forks to explore both paths, accumulating path conditions that constrain the symbolic inputs consistent with each path.

For vulnerability detection, symbolic execution enables systematic exploration of program paths and automatic generation of inputs that trigger specific behaviours. Given a property to check—such as the absence of integer overflow—symbolic execution explores paths, collects constraints under which the property is violated, and queries a satisfiability modulo theories (SMT) solver to determine whether satisfying inputs exist. If the solver finds a satisfying assignment, it provides a concrete exploit input; if the constraints are unsatisfiable, the property holds on that path~\cite{cadar2008exe}.

Symbolic execution faces the path explosion problem: the number of paths through a program grows exponentially with the number of branch points. Loops and recursive calls exacerbate this problem, potentially creating infinite paths. Practical symbolic execution engines employ various strategies to manage path explosion, including bounded exploration, state merging, and prioritised search~\cite{cadar2013symbolic}.

In the smart contract domain, tools such as Mythril~\cite{mueller2018smashing} and Manticore~\cite{mossberg2019manticore} employ symbolic execution to detect vulnerabilities. These tools symbolically execute EVM bytecode, modelling the Ethereum execution environment including storage, message calls, and environmental variables. When a path reaches a potentially vulnerable state—an external call with user-controlled destination, an arithmetic operation that may overflow—the tool checks whether the path is feasible and, if so, reports the vulnerability with a triggering input.

Environment modelling presents particular challenges for smart contract symbolic execution. The blockchain environment includes not just the contract under analysis but the entire ecosystem of other contracts, the block context, and the transaction parameters. Accurate modelling requires assumptions about what external contracts might do, what values environmental variables might take, and how transactions might be sequenced. Conservative assumptions lead to false positives; optimistic assumptions may miss real vulnerabilities~\cite{luu2016making}.

[https://www.cyfrin.io/blog/solidity-smart-contract-formal-verification-symbolic-execution#layer-4-or-formal-verification]

\subsection{Theorem Proving}
\label{subsec:theorem-proving}

Theorem proving represents the most powerful—and most demanding—approach to formal verification. Rather than exhaustively exploring state space, theorem proving directly constructs mathematical proofs that programs satisfy specifications. Interactive theorem provers such as Coq~\cite{bertot2013interactive}, Isabelle~\cite{nipkow2002isabelle}, and Lean~\cite{moura2021lean} provide frameworks for developing machine-checked proofs with human guidance.

The expressiveness of theorem proving is unmatched: any mathematically statable property can, in principle, be proven. This includes properties beyond the reach of model checking and symbolic execution, such as correctness relative to complex mathematical specifications or security properties involving cryptographic assumptions. Projects such as the Ethereum Foundation's formal specification efforts demonstrate the application of theorem proving to blockchain protocol verification~\cite{hirai2017defining}.

However, the cost of theorem proving is substantial. Proofs require significant human expertise to construct, often taking person-months of effort for non-trivial systems. The proof development process requires not just programming skill but mathematical sophistication and familiarity with the proof assistant's logic and tactics. For most smart contract development contexts, this cost-benefit trade-off renders full theorem proving impractical.

Nonetheless, theorem proving plays an important role in the broader smart contract security ecosystem. Proofs of foundational libraries, such as safe arithmetic operations or standard token implementations, can be developed once and reused widely. High-value contracts managing billions of dollars may justify the investment in formal proofs. The verification of compiler correctness ensures that properties proven at the source level are preserved through compilation~\cite{leroy2009formal}.


\section{Summary and Transition to LLM-based Analysis}
\label{sec:summary-transition}

This chapter has surveyed the foundational techniques for program analysis as applied to smart contract security. Static analysis, operating on representations from abstract syntax trees to interprocedural data flow, provides scalable vulnerability detection with soundness guarantees but may produce false positives. Dynamic analysis, through transaction tracing and fuzz testing, observes actual behaviours with precision but cannot guarantee absence of vulnerabilities. Formal verification offers mathematical certainty but at significant computational and human cost.

Each technique occupies a distinct point in the trade-off space between soundness, completeness, scalability, and automation. In practice, effective security analysis combines multiple techniques: static analysis for broad coverage and early detection, dynamic analysis for validation and edge case discovery, and selective formal verification for highest-assurance components. This defence-in-depth approach acknowledges that no single technique suffices for comprehensive security assessment.

Despite the sophistication of these techniques, significant challenges remain. The semantic gap between source code and natural language makes it difficult for automated tools to understand programmer intent, leading to both false positives (flagging intended behaviour as bugs) and false negatives (missing bugs that violate unstated invariants). The evolving landscape of smart contract patterns, from novel DeFi primitives to cross-chain bridges, continuously introduces new vulnerability classes that existing detectors may not recognise~\cite{werner2022sok}.

Recent advances in large language models (LLMs) offer a promising complementary approach. LLMs trained on vast corpora of code and natural language have demonstrated remarkable capabilities in code understanding, generation, and analysis~\cite{chen2021evaluating}. Their ability to process natural language specifications, understand contextual patterns, and generalise from examples positions them as potential contributors to security analysis workflows.

However, LLMs also exhibit significant limitations for security-critical applications. They may hallucinate non-existent vulnerabilities, miss subtle bugs, or provide confident but incorrect assessments~\cite{pearce2022asleep}. Their probabilistic nature provides no formal guarantees, and their sensitivity to prompt formulation introduces variability in outputs. Integrating LLMs into security analysis requires careful consideration of their strengths and limitations.

The subsequent chapters explore how LLM capabilities can augment traditional analysis techniques. We examine retrieval-augmented generation (RAG) architectures that ground LLM outputs in verified knowledge bases, reducing hallucination while preserving the flexibility and natural language understanding that LLMs provide. Wake-AI, our proposed system, demonstrates this integration, combining Wake's precise static analysis with LLM-based contextual understanding and explanation generation. This hybrid approach seeks to leverage the complementary strengths of symbolic and neural methods, advancing toward more effective, accessible, and reliable smart contract security analysis.


\chapter{Large Language Models}

The detection of vulnerabilities in smart contracts has traditionally relied on rule-based static analysis tools and formal verification methods. While these approaches have proven effective for certain classes of vulnerabilities, they struggle with complex logic bugs and require extensive manual effort to develop detection rules \cite{luu2016making, tsankov2018securify}. Recent advances in deep learning, particularly the emergence of transformer-based Large Language Models (LLMs), have opened new possibilities for automated vulnerability detection that can learn patterns from code rather than relying solely on pre-defined rules \cite{chen2021evaluating, wang2024software}.

This chapter explores the architectural foundations of modern LLMs and their application to vulnerability detection in smart contracts. We begin by tracing the evolution from early neural network approaches to the transformer architecture that underlies contemporary LLMs. Understanding these foundations is essential for our later discussion of how Wake-AI leverages LLM capabilities and why techniques like Retrieval-Augmented Generation (RAG) and Model Context Protocol (MCP) become necessary when working with the constraints of real-world code analysis tasks.

\section{Transformers}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/transformer.png}
    \caption{Transformer TODO REF.}
    \label{fig:transformer}
\end{figure}

Before diving into transformer architecture, it's worth briefly reviewing the path that led to their development. Early attempts to apply machine learning to code analysis relied on simpler models such as n-gram language models and basic recurrent neural networks \cite{hindle2012naturalness}. These approaches treated code as a sequence of tokens and attempted to learn statistical patterns that could distinguish vulnerable from secure code.

Recurrent Neural Networks (RNNs) and their more sophisticated variants, Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long}, represented a significant step forward. By maintaining hidden states that captured information about previously seen tokens, LSTMs could theoretically model long-range dependencies in code. However, they suffered from fundamental limitations. The sequential nature of RNN processing meant that information had to be passed through many intermediate states to connect distant parts of a program, leading to the vanishing gradient problem during training \cite{bengio1994learning}. More critically, the sequential processing constraint prevented parallelization, making it impractical to train on the massive code corpora needed for general-purpose code understanding.

The transformer architecture, introduced by Vaswani et al. in their seminal 2017 paper "Attention Is All You Need" \cite{vaswani2017attention}, fundamentally changed this landscape. By replacing sequential recurrence with attention mechanisms that could directly relate any two positions in a sequence, transformers eliminated the sequential bottleneck while providing more direct paths for gradient flow during training. This architectural shift enabled both the massive scale of modern LLMs and their superior ability to capture complex relationships in code.

\section{Transformer Architecture}

The transformer architecture consists of an encoder-decoder structure, though many modern LLMs use only the decoder portion (GPT-family models) or only the encoder portion (BERT-family models) \cite{radford2019language, devlin2019bert}. Figure~\ref{fig:transformer} illustrates the complete transformer architecture. For vulnerability detection tasks, we primarily work with decoder-only models that have been pre-trained on large code corpora and can generate natural language explanations of detected issues.

At its core, a transformer processes sequences through a series of identical layers, each containing two main components: a multi-head self-attention mechanism and a position-wise feed-forward network. The architecture also employs residual connections around each of these sub-layers, followed by layer normalization \cite{ba2016layer}.

The input to the transformer begins with token embeddings that represent each element of the input sequence as a dense vector. For code analysis, these tokens might represent keywords, identifiers, operators, and other syntactic elements of the programming language. Crucially, since the attention mechanism itself has no inherent notion of sequence order, positional encodings are added to these embeddings to inject information about token positions.

\subsection{Self-Attention Mechanism}

The self-attention mechanism is the defining innovation of transformer architecture. Unlike RNNs that process tokens sequentially, self-attention allows the model to directly compute relationships between all pairs of tokens in parallel. This capability is particularly valuable for code analysis, where understanding a vulnerability often requires relating elements that are far apart in the token sequence—for example, connecting a variable declaration to its use in a security-critical operation several functions away.

The attention mechanism operates through three learned linear projections of the input: queries (Q), keys (K), and values (V). For each position in the sequence, the model computes attention scores that determine how much focus to place on every other position. Mathematically, the attention operation for a single head is defined as:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $d_k$ is the dimension of the key vectors \cite{vaswani2017attention}. The scaling factor $\sqrt{d_k}$ prevents the dot products from growing too large in magnitude, which would push the softmax function into regions with extremely small gradients.

To understand this intuitively, consider analyzing a function call in Solidity code. The query represents "what am I looking for" from the perspective of each token. The keys represent "what information do I contain" from each token's perspective. The dot product $QK^T$ computes compatibility scores between queries and keys—essentially asking "how relevant is each token to each other token?" The softmax normalizes these scores into a probability distribution, and finally, this distribution is used to take a weighted average of the values, producing an attention-weighted representation.

Multi-head attention extends this by running several attention operations in parallel, each with different learned projection matrices. The outputs are concatenated and linearly transformed:

\begin{equation}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}

where each $\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$ \cite{vaswani2017attention}. This multi-head design allows the model to attend to information from different representation subspaces simultaneously. In practice, different attention heads often learn to capture different types of relationships—some might focus on syntactic structure, while others capture semantic relationships or data flow patterns \cite{clark2019does, vig2019analyzing}.

For vulnerability detection, this multi-head attention proves particularly powerful. One attention head might learn to track variable definitions and uses, another might focus on function call chains, and yet another might specialize in recognizing common vulnerability patterns. The model can then combine these different perspectives to make more nuanced judgments about potential security issues.

The transformer architecture includes two variants of attention: in the encoder, self-attention allows each token to attend to all tokens in the input sequence. In the decoder, masked self-attention restricts each token to only attend to previous tokens (and itself), which is essential for autoregressive generation tasks. When we use a pre-trained LLM for vulnerability analysis, we're typically leveraging this masked attention structure—the model has learned to predict subsequent tokens given previous context, and this predictive capability extends to understanding what code patterns might indicate vulnerabilities.

\subsection{Positional Encoding and Context Windows}

A fundamental characteristic of the attention mechanism is its permutation invariance: if we shuffled the input tokens, the attention computation would produce the same results (just in shuffled order). For natural language and code, where token order carries critical semantic information, this is clearly problematic. Positional encoding solves this by adding position-dependent signals to the token embeddings before they enter the transformer layers.

The original transformer used sinusoidal positional encodings \cite{vaswani2017attention}:

\begin{equation}
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{equation}
\begin{equation}
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{equation}

where $pos$ is the position in the sequence, $i$ is the dimension index, and $d_{model}$ is the model dimension. This encoding has useful mathematical properties: any fixed offset in position corresponds to a linear transformation in the encoding space, potentially allowing the model to learn to attend by relative positions. Moreover, these functions can extend to sequence lengths beyond those seen during training.

However, many modern LLMs use learned positional embeddings instead \cite{devlin2019bert, radford2019language}. While these must be explicitly trained for each position up to a maximum sequence length, they offer more flexibility and often perform slightly better in practice. More recent approaches like Rotary Position Embedding (RoPE) \cite{su2021roformer} and ALiBi \cite{press2022train} have shown promise in extending effective context lengths beyond training limits, an important consideration for analyzing long smart contracts.

The concept of a \textit{context window} refers to the maximum number of tokens a model can process at once. This is a hard constraint imposed by both the positional encoding scheme and computational resources—attention has $O(n^2)$ complexity with sequence length $n$, making it expensive for very long sequences. Early transformer models worked with context windows of 512 or 1024 tokens \cite{vaswani2017attention, devlin2019bert}. Modern LLMs have dramatically expanded this: GPT-4 supports context windows up to 128,000 tokens \cite{openai2023gpt4}, and Claude 3 extends to 200,000 tokens \cite{anthropic2024claude}.

For smart contract analysis, context window limitations pose a significant practical challenge. A typical smart contract might contain 500-2000 lines of Solidity code, which can translate to 5,000-20,000 tokens once we include relevant context like imported libraries, interface definitions, and documentation. More complex projects might exceed even generous context limits. This constraint motivates the need for Retrieval-Augmented Generation (RAG) approaches, which we explore in the next chapter. Rather than feeding an entire codebase into the model at once, RAG systems selectively retrieve the most relevant code segments, allowing the LLM to focus its limited context window on the information most likely to contain or explain vulnerabilities.

The feed-forward networks in each transformer layer also play a crucial role, though they're often overshadowed by the attention mechanism in discussions. After attention aggregates information across the sequence, the feed-forward network applies the same learned transformation to each position independently:

\begin{equation}
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\end{equation}

This two-layer network with ReLU activation significantly expands the model's capacity to learn complex patterns \cite{vaswani2017attention}. Recent work suggests these feed-forward layers serve as "key-value memories" where the model stores factual knowledge learned during pre-training \cite{geva2021transformer}.

Understanding these architectural components is essential for effectively applying LLMs to vulnerability detection. The attention mechanism's ability to capture long-range dependencies helps identify vulnerabilities that involve interactions between distant parts of code. The multi-head design allows specialization for different types of patterns. And the context window constraints explain why we need sophisticated retrieval strategies to handle real-world smart contracts. In the following chapters, we examine how these capabilities and limitations shape our approach to building Wake-AI, a system that combines static analysis tools like Wake with the pattern-recognition abilities of modern LLMs.


\section{Training paradigms}
\subsection{Pre-training}
\label{subsec:llm-pretraining}
Most LLMs are first \emph{pre-trained} on large corpora using self-supervised objectives that require no manual labeling. In decoder-only models, the dominant objective is next-token prediction: given a prefix, the model learns to predict the distribution of the next token. In encoder-only models such as BERT, masked language modeling instead trains the model to recover randomly masked tokens from surrounding context \cite{devlin2019bert}. Although the objectives differ, both encourage the model to build internal representations that capture statistical regularities in language and code.

For code, pre-training has two important implications. First, it teaches models syntax and idioms that are shared across many repositories, which helps with code comprehension and generation. Second, it also absorbs common-but-not-necessarily-correct patterns from open-source code, including insecure practices. This matters in security contexts: an LLM's familiarity with typical code can help it spot deviations or suspicious constructs, but pre-training alone does not provide guarantees that its conclusions are correct.

\subsection{Fine-tuning}
\label{subsec:llm-finetuning}
After pre-training, models are often adapted to downstream tasks via \emph{fine-tuning}. A common approach is supervised fine-tuning on instruction-response pairs so the model follows user intent more reliably. Many production models also incorporate alignment techniques such as reinforcement learning from human feedback (RLHF), which optimizes model outputs toward human preference judgments \cite{ouyang2022training}.

In security auditing, fine-tuning can improve usefulness in two ways. It can bias the model toward structured reporting (clear findings, evidence, remediation steps), and it can improve adherence to constraints (e.g., ``only claim what you can justify''). However, fine-tuning also risks overfitting to narrow datasets or encoding dataset-specific biases. For this thesis, we therefore treat fine-tuning as complementary rather than required: grounding and tool integration can improve reliability even when using general-purpose models.

\subsubsection*{Reinforcement Learning from Human Feedback}
[CITE] Reinforcement Learning from Human Feedback (RLHF) is a machine learning paradigm
that combines elements of reinforcement learning and supervised learning to enable
AI systems to learn and make decisions in a more human-aligned manner [63]. RLHF
introduces human feedback as a valuable source of guidance, which can help AI systems navigate complex decision spaces, align with human values, and make more
informed and ethical choices [64].
Fig. 2.4 illustrates the RLHF process, which consists of several key steps. Firstly,
prompts are collected to train a reward model through human labelling. Subsequently, the language model is fine-tuned using reinforcement learning techniques,
followed by deploying and iterating the model [65]. During the fine-tuning process,
the reward model guides the model’s actions, and the agent aims to maximize cumulative rewards based on the reward model’s predictions.

\subsection{Prompting}
\label{subsec:llm-prompting}
\emph{Prompting} refers to shaping model behavior by providing instructions, context, and examples at inference time rather than changing model weights. Few-shot prompting, popularized by large-scale decoder-only models, demonstrates that including a small number of input-output examples can elicit task behavior without explicit fine-tuning \cite{brown2020language}. For code auditing, prompts often include: (i) a description of the vulnerability class to look for, (ii) the relevant code snippets, and (iii) an expected output format such as a checklist or a structured report.

Prompting is powerful but fragile. Small changes in wording can lead to different conclusions, and prompts that include large amounts of irrelevant context can degrade output quality. These realities motivate the later design of retrieval-augmented prompting: instead of relying on a single large prompt, we programmatically retrieve and assemble focused evidence that the model can cite and reason over.

\subsubsection*{Prompting strategies}
\begin{itemize}
    \item \textbf{Task decomposition} - breaks an analysis into smaller sub-tasks such as ``summarize the contract's assets and trust boundaries'', ``identify external calls and state updates'', and ``check whether updates happen before interactions''. Decomposition improves reliability because each sub-task has narrower scope and clearer success criteria, and it naturally aligns with tool usage: static analysis can enumerate call sites, while the LLM focuses on explaining why a pattern is risky and what remediation looks like.
    
    \item \textbf{Chain-of-thought} - elicits intermediate reasoning steps and can improve performance on multi-step tasks by encouraging the model to keep track of assumptions and partial conclusions \cite{wei2022chain}. In security settings, however, the goal is not to obtain long informal traces, but to obtain \emph{checkable} reasoning: claims should be tied to specific code locations or retrieved evidence. In later chapters, we therefore structure outputs around findings, evidence, and remediation rather than relying on free-form reasoning traces.
    
    \item \textbf{Few-shot learning} - calibrates what ``good'' output looks like. In auditing, examples are most valuable when they mirror the desired report structure (severity, impact, evidence, remediation) and when they demonstrate the level of conservatism expected: the model should prefer ``uncertain, needs evidence'' over confident speculation. This style is compatible with retrieval: retrieved examples become dynamic few-shot demonstrations that are tailored to the code under analysis rather than static prompt templates.
\end{itemize}

\section{Code-Specialized Models and Embeddings}
% Explain why code models/embeddings help retrieval + similarity search (ties directly to your KB).
\subsection{Embeddings}
\subsection{Encoder/Decoder Models}
%codebert
\subsection{Seq2seq Models}
% T5/CodeT5 family

\section{LLMs in Smart Contract Auditing}
% Focus on 4 buckets: prompt-only, agentic, fine-tuned, RAG-augmented.
\subsection{Prompt-Only and Agentic Auditing Pipelines}
% Example: multi-agent approaches; cite one solid paper you actually discuss.
\subsection{Fine-tuned Models for Smart Contract Vulnerability Detection}
% Example: SmartLLM (RAG + fine-tuning). :contentReference[oaicite:15]{index=15}
\subsection{RAG-Augmented Detection Frameworks}
% Discuss ParaVul (explicit RAG + verification/fusion). :contentReference[oaicite:16]{index=16}
% (This directly motivates your “RAG inside Wake-AI” choice.)

\section{Challenges and Limitations}
\label{sec:llm-challenges}

\subsection{Hallucinations}
\label{subsec:hallucinations}
LLMs can produce plausible-sounding but incorrect statements, including claims about vulnerabilities that do not exist in the provided code. This is particularly problematic in auditing, where false positives waste reviewer time and may lead to unnecessary code changes. Empirical studies of code models report non-trivial rates of insecure or incorrect suggestions, especially when prompts are underspecified or when the model lacks relevant context \cite{pearce2022asleep}. A practical mitigation is to require evidence: every finding should reference concrete code behavior, tool outputs, or retrieved documentation rather than relying on model intuition alone.

\subsection{Context Window}
\label{subsec:context-window}
Even with large context windows, real smart contract systems can exceed what is reasonable to include in a single prompt, especially when imported libraries, interfaces, and deployment configuration matter. Moreover, models have a fixed training cutoff; new vulnerability classes and best practices emerge continuously. RAG partially addresses both issues by retrieving up-to-date, task-relevant information at inference time and by focusing the prompt on a small set of high-signal chunks \cite{lewis2020retrieval}. The remaining challenge is selection: retrieval must be precise enough to avoid overwhelming the model with irrelevant text.

\subsection{Security}
\label{subsec:security}
% This is where you justify grounding with RAG + structured tool calls (MCP).
Audits are security-critical, so outputs must be reproducible and traceable. Purely generative answers are difficult to verify after the fact, and different runs may yield different results. Tool-augmented approaches improve reproducibility by logging which code artifacts and documents were used, and by separating verifiable computations (e.g., static analysis queries) from probabilistic summarization.

When agents can call external tools, additional safety concerns arise, such as prompt injection in retrieved documents and unintended execution of dangerous actions. For our setting, we mitigate these risks by restricting the agent's tool surface to read-only analysis and retrieval primitives and by treating all retrieved text as untrusted input that must not override analysis policies. [CITATION NEEDED]

LLMs offer a complementary capability to traditional program analysis: they can translate code behavior into natural language, relate patterns to prior examples, and assist in drafting remediation guidance. However, these strengths come with constraints that are especially visible in smart contract auditing: limited context windows, non-deterministic behavior, and the risk of hallucinated findings.

After understanding what is blockchain and ethereum, how code analysis looks lie and what LLMS are capable of, we are ready to move to Wake framework, which incorporates all mentioned before.

\chapter{The Wake Framework}
\label{chap:wake}

This chapter provides a fundamental overview of Wake framework. It focuses on its internal model and analysis workflow. After it, the separate module with AI integration is described with findings of limitations. [TODO]

\section{Overview}
\label{sec:wake-overview}

Wake is an open source Python-based development, testing, and security analysis framework specifically designed for Solidity smart contracts~\cite{wake-docs} [TODO how to correctly cite this docs]. Developed by Ackee Blockchain Security\footnote{\url{https://ackee.xyz/}}, Wake addresses the critical need for comprehensive tooling in the blockchain security ecosystem. 

The framework provides several core capabilities:

\begin{itemize}
    \item \textbf{Testing Framework}: Built on top of \texttt{pytest}~\cite{pytest}, Wake enables developers to write unit tests and integration tests for smart contracts using Python's familiar testing idioms.

    \item \textbf{Property-Based Fuzzer}: Wake includes a sophisticated fuzzer supporting both property-based and manually-guided fuzzing strategies to discover edge cases and vulnerabilities through automated input generation.

    \item \textbf{Vulnerability Detectors}: The framework ships with built-in detectors for common vulnerability patterns such as reentrancy, integer overflows, and logic flaws. These detectors are designed with a low false-positive philosophy to minimize manual investigation overhead during audits.

    \item \textbf{Printers}: Information extraction tools that generate structured outputs about contract properties, including control flow graphs, inheritance hierarchies, and data dependencies.

    \item \textbf{Development Tools}: Wake integrates with modern development workflows through a Language Server Protocol (LSP) implementation and a Visual Studio Code extension called ``Tools for Solidity''~\cite{tools-for-solidity}, providing real-time feedback on vulnerabilities and code quality.

    \item \textbf{Deployment and Interaction}: Support for mainnet interactions and contract deployments, enabling seamless transition from testing to production.
\end{itemize}

Wake has been actively used by Ackee Blockchain Security in professional smart contract audits, where it has helped discover numerous high and critical severity vulnerabilities in protocols such as IPOR Protocol\footnote{\url{https://www.ipor.io/}}, PWN Protocol\footnote{\url{https://pwn.xyz/}}, Brahma Console\footnote{\url{https://console.brahma.fi/}}, and Lido Community Staking Module\footnote{\url{https://csm.lido.fi/}} \cite{wake-docs}.


\section{Internal Model}
\label{sec:wake-internal-model}

Wake constructs a rich internal representation of Solidity smart contracts that serves as the foundation for all analysis capabilities. Because of Wake build base on static analysis, all its components were described in Section \ref{sec:static-analysis}.

\subsubsection*{Intermediate Representation}
\label{subsubsec:wake-ir}

At the core of Wake's analysis capabilities lies a typed intermediate representation (IR) derived from the Solidity compiler's Abstract Syntax Tree (AST). The IR provides a structured view of the source code, representing all language constructs. The IR preserves type information and cross-references between declarations and their usages, enabling precise semantic analysis. Each IR node maintains references to its parent and children, facilitating traversal in any direction. Wake's IR is fully typed using Python's type system, ensuring type safety during analysis development.


\subsubsection*{Control Flow Graph}
\label{subsubsec:wake-cfg}

Wake constructs CFGs for functions and modifiers, representing the possible execution paths through the code. It provides utility functions for reachability analysis, such as determining whether the start node is reachable from a given position (backward reachability), or whether the success/revert end nodes are reachable from a statement (forward reachability). These primitives are essential for detecting issues like dead code or functions that always revert.

\subsubsection*{Data Dependency Graph}
\label{subsubsec:wake-ddg}

The DDG is one of Wake's most sophisticated analysis structures, tracking how data flows through the contract. It automatically expands composite types (structs, arrays, mappings) to track data flow at the member/element level. For example, when a struct is assigned, the DDG creates edges not only for the struct itself but also for each of its members recursively.

\section{Analysis Instruments}
\label{sec:wake-analysis}

Building upon the internal model, Wake provides a comprehensive infrastructure for implementing various analysis techniques.

\subsubsection*{Detectors}
\label{subsubsec:wake-detectors}

Wake's static analysis framework enables the implementation of custom vulnerability detectors. It already covers patterns like reentrancy, unsafe ERC-20 call, unused constructions and many others\footnote{\url{https://ackee.xyz/wake/docs/latest/static-analysis/using-detectors/}}. Each detector is a Python class that:

\begin{enumerate}
    \item Receives access to the compiled IR and associated graphs (CFG, DDG).
    \item Traverses relevant code structures to identify potential issues.
    \item Reports findings with precise source locations and severity classifications.
\end{enumerate}

\subsubsection*{Fuzz Testing}
\label{subsubsec:wake-fuzzing}

Complementing static analysis, Wake provides robust support for dynamic testing and fuzzing:

\begin{itemize}
    \item \textbf{pytest Integration}: Tests are written as standard Python functions using pytest fixtures and assertions, lowering the barrier for developers familiar with Python testing.

    \item \textbf{Property-Based Fuzzing}: Wake's fuzzer generates diverse inputs to test invariants and properties, automatically exploring edge cases that manual testing might miss.

    \item \textbf{Stateful Fuzzing}: Support for fuzzing that maintains state across operations, essential for testing complex DeFi protocols with multi-step interactions.

    \item \textbf{Shrinking}: When the fuzzer finds a failing input, it attempts to minimize the input to the smallest case that still reproduces the failure, simplifying debugging.
\end{itemize}

Wake has been used to discover critical vulnerabilities through fuzzing in production protocols. The framework's test collection ``Awesome Wake Tests''~\cite{awesome-wake-tests} provides reference implementations demonstrating effective fuzzing strategies.


\subsubsection*{Printers}
\label{subsubsec:wake-printers}

Wake includes auxiliary tools that support development and audit workflows:

\begin{itemize}
    \item \textbf{Printers}: Extract and visualize structural information from contracts:
    \begin{itemize}
        \item Contract cross-reference graphs showing relationships between contracts
        \item Data dependency graph visualization using Graphviz
        \item Contract size estimation for deployment gas optimization
        \item Contract summaries and function signature extraction
        \item C3 linearization visualization for inheritance analysis
    \end{itemize}

    \item \textbf{Language Server Protocol (LSP)}: Real-time analysis integration with editors, providing:
    \begin{itemize}
        \item On-the-fly vulnerability detection
        \item Code navigation and go-to-definition
        \item Hover information with type details
        \item Variable DDG visualization within the editor
    \end{itemize}

    \item \textbf{Compiler Management}: Built-in \texttt{solc} version management for reproducible builds across different Solidity versions.

    \item \textbf{Code Flattening}: Utility for combining multi-file contracts into a single file for verification on block explorers.
\end{itemize}


\section{Wake-AI}
\label{sec:wake-ai}

With fast developing of tehcnologies, growing LLM capabilities and often their usage in practice, as we were able notice in Section \ref{sec:}, LLMs are integrated into Wake as special separate tool aimed to comprehensive AI audit of smart contracts. Unfortunateylly, challneges of static analysis described above, restricted a lo of spce and other rtypes of vulnerabilities. So there is time, when huge language models come to help auditors with security analysis. 

\subsection{Architecture}
\label{subsec:wake-ai-architecture}

Wake-AI extends the Wake framework with artificial intelligence capabilities for automated security auditing. It implements a multi-stage workflow architecture where each stage performs a specific analysis task. The system orchestrates AI models through structured sessions:

\begin{itemize}
    \item \textbf{Session Management}: Wake-AI supports multiple AI backends, including Claude (via MCP---Model Context Protocol) and OpenAI Codex models. Sessions maintain conversation context and can be forked for parallel processing.

    \item \textbf{MCP Integration}: Wake exposes its analysis capabilities as MCP tools, allowing AI models to query the IR, retrieve function signatures, access CFG information, and invoke static analysis detectors during their reasoning process.

    \item \textbf{Working Directory}: Each audit maintains a structured working directory containing intermediate results, allowing workflow resumption and result inspection.

    \item \textbf{Progress Tracking}: The framework provides real-time progress updates and token usage tracking for cost management.

    \item Prompt-Based Reasoning and Validation
\end{itemize}


\subsection{Limitations}
\label{subsec:wake-ai-limitations}

Despite its capabilities, Wake-AI faces several limitations:

\begin{itemize}
    \item \textbf{Context Window Constraints}: LLMs have limited context windows, restricting the amount of code and analysis data that can be processed simultaneously. Large codebases may require chunking strategies that could miss cross-file vulnerabilities.

    \item \textbf{Hallucination Risk}: AI models may generate plausible-sounding but incorrect vulnerability reports. The triage phase mitigates this through evidence requirements, but some false positives may persist.

    \item \textbf{Determinism}: AI model outputs are inherently non-deterministic, potentially producing different findings on repeated runs of the same codebase.

    \item \textbf{Knowledge Cutoff}: Models may lack awareness of recently discovered vulnerability patterns or new Solidity language features introduced after their training data cutoff.

    \item \textbf{Limited Tool Access}: While MCP integration provides access to Wake's analysis tools, the AI cannot execute arbitrary code or perform dynamic testing during analysis. Complex vulnerabilities requiring runtime behavior observation may be missed.
\end{itemize}

Some of these limitations merit attention and can be addressed through alternative approaches. Others, such as hallucination risk and non-determinism, cannot be fully resolved due to the inherent nature of LLMs. This thesis focuses on one of the most critical limitations: knowledge cutoff. The following chapter presents a proposed solution designed to increase the detection of genuine security issues while reducing false positives. This is achieved by providing the LLM with similar historical issues, enabling it to make more informed judgments about vulnerabilities.


\chapter{Proposed Methodology}
% THIS is your main “design contribution” chapter.
\section{Design Goals}


\section{Retrieval augmented generation systen}

A recent study by Lewis et al. [2020] investigates advancements in RAG models. They detail the architecture and key strategies employed in RAG, which
aim to leverage the strengths of LLMs while simultaneously mitigating their
limitations. 

he RAG framework comprises of two primary components: a retriever in
Equation 2.1 and a generator in Equation 2.2, parameterized by η and θ respectively. The retriever retrieves (topk truncated) distributions over text passages given an input query x, while the generator generates the next token
based on the context of preceding tokens, the input sequence, and a retrieved
passage.
pη(z|x) (2.1)
• η: Non-parametric retriever.
• x: Sequence given by user.
• z: Text passages to retrieve.
pθ(yi
|x, z, y1:i−1) (2.2)
• θ: Parametric generator.
• yi
: Target sequence to generate.
• y1:i−1: Sequences previously generated.


% Cite RAG paper for concept framing. :contentReference[oaicite:21]{index=21}
\subsection{Query Generation}
% How you create retrieval queries: from detectors, IR features, function summaries, etc.
\subsection{Re-ranking}
% How you select top-k, de-duplicate, and compress into prompt context.


% Goals: groundedness, audit usefulness, token efficiency, modularity, reproducibility.
\section{System Architecture Overview}
% Diagram: Wake-AI steps + retrieval module + KB + MCP server + LLM.
% Cite MCP intro/spec. :contentReference[oaicite:19]{index=19}
\subsection{High-Level Workflow}

\section{Knowledge Base}
Describe solodit, that it will be taken from here, placed their all data with labels, metadata, crossing and mainly embed by summaries with normalization, Also there is a part with code + private reports from ackee audits

\subsection{Indexing and Embeddings}
% How documents are chunked + embedded for retrieval.
TODO

\section{Model context protocol integration}
\subsection{Endpoints and Contracts}
% Define each endpoint (search patterns, fetch details, get examples, etc.) with expected inputs/outputs.
\subsubsection*{Security considerations}
% Timeouts, empty results, injection resistance, deterministic logging.

\section{Prompt engineering}
\subsection{TODO}

\section{Chapter Summary and Transition to Implementation}
% 5–8 lines: design choices -> now show how it’s implemented inside Wake-AI.

\chapter{Evaluation}

\section{Dataset}

% Goal: show it works and understand when/why.
\section{Evaluation Questions}
% RQ1: Does RAG reduce hallucinations/false positives? RQ2: Does it improve coverage? RQ3: Cost/latency tradeoffs?
\section{Experimental Setup}
\subsection{Benchmarks and Datasets}
% Real vulnerable contracts + (optional) synthetic cases to test each pipeline component.
\subsection{Baselines}
% Wake-AI (prompt-only), Wake detectors, and Slither as external baseline. :contentReference[oaicite:22]{index=22}
\subsection{Metrics}
% Precision/recall per vulnerability class; “evidence correctness”; runtime/token cost.

\section{Experiments}
\subsection{Retrieval Quality (Component Test)}
% Top-k hit rate / qualitative relevance checks of retrieved patterns.
\subsection{End-to-End Vulnerability Detection}
% Compare baseline vs RAG-augmented on same contracts.
\subsection{Ablation Study}
% Remove reranking / remove audit KB / remove detector-seeded queries to show what matters.
\subsection{Qualitative Case Studies and Error Analysis}
% 2–3 focused cases: success, failure, “helped explanation but not detection,” etc.

\section{Threats to Validity}
% Dataset bias, label quality, prompt sensitivity, reproducibility, and generalization.
\section{Summary and Transition to Conclusion}
% 5–8 lines: what improved, what didn’t, and why.

\chapter{Conclusion}
\section{Summary of Contributions}
% One tight list: RAG+MCP integration, KB design, Wake-AI extension, empirical findings.
\section{Limitations}
% Where it fails: KB gaps, retrieval noise, complex protocol logic, environment assumptions.
\section{Future Work}
% Strong future directions: tighter static/IR-guided retrieval, symbolic checks, richer audit KB, standardized benchmarks.