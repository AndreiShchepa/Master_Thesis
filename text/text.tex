
% uncomment the following line to create an unnumbered chapter
\chapter*{Introduction}\addcontentsline{toc}{chapter}{Introduction}\markboth{Introduction}{Introduction}
%---------------------------------------------------------------
\setcounter{page}{1}

Problem, motivation, goal, contributions, and thesis roadmap.
Why smart contract bugs are high impact (immutability, financial incentives) and why existing audits/tools still miss logical flaws. Tell about security audits and how it is time consuming. Then a bit about LLM, which is developed in parallel.
State your contribution precisely: extending Wake-AI with MCP-based RAG + building KB from Solidity vulns/audit knowledge.
One paragraph explaining what each chapter delivers and how it connects. (ethereum, code analysis, large language models, wake framework, proposes methododology, implementation, evaluation, conclusion)

In June 2016, an attacker exploited a reentrancy vulnerability in The DAO smart contract, draining approximately \$60 million worth of Ether in a matter of hours \cite{mehar2019understanding}. The technical flaw was subtle—a recursive call that allowed withdrawals before balance updates—but the consequences were catastrophic. This incident, along with numerous subsequent exploits in decentralized finance (DeFi) protocols, highlights a fundamental challenge in blockchain development: once deployed, smart contracts are immutable, and vulnerabilities cannot be easily patched. Unlike traditional software where bugs might cause inconvenience or data corruption, smart contract vulnerabilities directly translate to financial losses, often measured in millions of dollars \cite{atzei2017survey, perez2021smart}.

The immutability of blockchain systems creates a paradox for security. While immutability is precisely what makes smart contracts trustworthy—users can verify that contract logic won't change arbitrarily—it also means that any security flaw becomes permanent. An attacker who discovers a vulnerability can exploit it with confidence, knowing the code cannot be modified to prevent the attack. Moreover, the financial nature of most smart contract applications creates strong economic incentives for exploitation. When a DeFi protocol manages hundreds of millions in assets, even sophisticated attackers invest significant resources in finding vulnerabilities. This high-stakes environment has driven the development of increasingly rigorous security audit practices, yet critical vulnerabilities continue to slip through.

Current smart contract security relies heavily on manual code audits supplemented by automated analysis tools. Professional audit firms employ expert security researchers who spend weeks examining contract code, looking for common vulnerability patterns, reasoning about edge cases, and attempting to violate protocol invariants \cite{perez2021smart}. Static analysis tools like Slither \cite{feist2019slither} and Mythril \cite{mythril} automate detection of known vulnerability patterns such as reentrancy, unchecked external calls, and integer overflows. Fuzz testing tools like Echidna \cite{grieco2020echidna} explore program state spaces to find invariant violations. Despite these sophisticated techniques, significant vulnerabilities continue to evade detection until they are exploited in production.

The fundamental limitation is that many critical vulnerabilities are not simple pattern matches but rather logical flaws that emerge from complex interactions between contract components or violations of implicit business logic assumptions. For example, a flash loan attack might exploit the interaction between a pricing oracle, a lending pool, and a liquidation mechanism in ways that are individually correct but collectively create arbitrage opportunities. Detecting such vulnerabilities requires understanding not just what the code does syntactically, but what it means semantically in the context of the broader protocol design. This type of reasoning has historically required human expertise, making audits time-consuming and expensive. [CITATION NEEDED]

Recent advances in Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and reasoning about code. Models trained on vast corpora of source code can explain code functionality, identify potential bugs, and generate candidate implementations when properly prompted \cite{openai_gpt41,anthropic_context_windows}. However, applying LLMs directly to smart contract security faces several challenges. First, LLMs have limited context windows, so they cannot ingest arbitrarily large codebases and documentation in a single pass. Second, LLMs lack access to the structured analysis results produced by static analysis tools, which provide precise information about control flow, data dependencies, and detected patterns. Third, effective vulnerability analysis requires domain-specific knowledge about Solidity-specific pitfalls, common attack vectors, and audit best practices that may not be well-represented in general code training data.

This thesis addresses these challenges by developing an integrated system that combines the static analysis capabilities of the Wake framework with the reasoning abilities of LLMs through a Model Context Protocol (MCP)-based architecture. Wake is an advanced Python-based framework for Solidity analysis that provides rich programmatic access to abstract syntax trees, control flow graphs, and custom vulnerability detectors \cite{wake2023documentation}. By augmenting Wake with LLM-based analysis through MCP—a protocol that enables structured tool use by language models—we create a system where the LLM can request specific analysis results, retrieve relevant code segments, and access domain knowledge as needed, rather than requiring all context upfront.

% \section*{Contributions}

% This thesis makes the following contributions:

% \textbf{1. MCP-based RAG Architecture for Code Analysis:} We design and implement a Retrieval-Augmented Generation system that enables LLMs to effectively analyze large smart contract codebases despite context window limitations. Our architecture uses the Model Context Protocol to provide the LLM with structured access to code segments, static analysis results, and domain knowledge. Unlike naive approaches that attempt to fit entire contracts into context, our system allows the LLM to iteratively retrieve relevant information based on its analysis needs, enabling it to handle real-world DeFi protocols that span dozens of contracts and tens of thousands of lines of code.

% \textbf{2. Domain-Specific Knowledge Base:} We construct a curated knowledge base of Solidity vulnerability patterns, audit methodologies, and security best practices drawn from public audit reports, vulnerability databases like the Smart Contract Weakness Classification (SWC), and documentation of historical exploits. This knowledge base is structured to support efficient retrieval and provides the LLM with specialized information about blockchain-specific attack vectors that may not be adequately represented in its pre-training data. We develop embedding strategies and retrieval mechanisms specifically tuned for technical security content.

% \textbf{3. Wake Framework Integration:} We extend the Wake-AI project with MCP server implementations that expose Wake's analysis capabilities to LLM agents. This integration allows the language model to request specific analysis operations—such as computing data flow for a variable, identifying all functions that modify a particular state variable, or retrieving functions that match specific structural patterns—and receive structured results that inform its reasoning. The MCP-based design ensures that the integration remains modular and extensible to additional analysis tools.

% \textbf{4. Empirical Evaluation:} We evaluate our system on a dataset of real smart contracts with known vulnerabilities, measuring both detection accuracy and the quality of vulnerability explanations. Our evaluation examines how effectively the RAG-augmented LLM identifies different vulnerability classes compared to baseline static analyzers and explores the system's ability to explain detected issues in ways useful to human auditors. We also analyze failure modes to understand the limitations of the LLM-based approach.

The first chapter provides the necessary background on blockchain technology, the Ethereum platform, and smart contract development. It also establishes the domain context and security challenges that motivate this work.

The second chapter surveys existing approaches to code analysis like static analysis, dynamic testing and formal verification.

The third chapter explores the application of LLMs to code understanding and security analysis. We explain the transformer architecture that underlies modern language models, discuss how these models are adapted for code through pre-training and fine-tuning, and examine their capabilities and limitations for vulnerability detection. 

The remainder of the chapters focus on the design of the methodology proposed that was described above, its implementation and evaluation.

% \textbf{Chapter 5: Retrieval-Augmented Generation and Tool Use} introduces the key techniques that enable LLMs to work effectively with large codebases and specialized tools. We explain Retrieval-Augmented Generation (RAG) architectures, embedding strategies for code, and the Model Context Protocol that allows LLMs to orchestrate external tools. This chapter provides the conceptual framework for our system design.

% \textbf{Chapter 6: Methodology} presents our approach in detail. We describe the architecture of our MCP-based RAG system, the design of our domain knowledge base, the integration with Wake's analysis capabilities, and the retrieval strategies we employ. This chapter explains the key design decisions and trade-offs in building a practical LLM-augmented audit system.

% \textbf{Chapter 7: Implementation} details the concrete realization of our design. We discuss the software architecture, MCP server implementations, database schemas, and integration points between components. This chapter provides sufficient detail for replication and extension of our work.

% \textbf{Chapter 8: Evaluation} presents our experimental results. We describe our dataset of vulnerable contracts, evaluation metrics, baseline comparisons, and analysis of system performance across different vulnerability classes. We examine both quantitative metrics like precision and recall as well as qualitative aspects like explanation quality and practical utility for auditors.

The last chapter summarizes contributions of this work, discusses limitations and results of the proposed approach, and outlines directions for future work.

The goal of this thesis is not to replace human auditors with automated systems—the creative reasoning and contextual understanding required for comprehensive security analysis remains a fundamentally human capability. Rather, we aim to augment auditors with intelligent tools that handle the mechanical aspects of information retrieval and pattern recognition, allowing human expertise to focus on the complex reasoning tasks where it adds the most value. By combining the precision of static analysis, the knowledge retrieval of RAG systems, and the pattern recognition of LLMs, we take a step toward making smart contract security more accessible and effective.

\chapter*{Objectives}

Tell a bit about what was done and list objectives that were fullfilled:

\begin{itemize}
    \item Common vulnerability patterns and existing auditing methodologies of Ethereum smart contracts were studied.
    \item A state of the art review on the integration of Large Language Models for automated reasoning and the development of AI-based auditing agents was conducted.
    \item A system that integrates an LLM into the security auditing workflow was designed with further implementation, testing and evaluation.
    \item The obtained results, limitations, and potential improvements of LLM-based auditing systems were discussed in the end.
\end{itemize}


\chapter{Ethereum}
This chapter introduces the Ethereum foundations required for the remainder of this thesis. It starts with a brief recap of blockchain structure and consensus, then focuses on Ethereum's execution model through the Ethereum Virtual Machine (EVM) and the Solidity programming language. Finally, it summarizes common smart-contract vulnerability categories that repeatedly appear in practical audits and will later inform the knowledge base and retrieval strategies used by our tooling.

\section{Overview}
Ethereum extends the ledger-centric model of early blockchains into a general-purpose execution platform for smart contracts and decentralized applications (dApps). The idea was first articulated by Buterin in 2013 \cite{Buterin2013} and formalized as a replicated state-transition system in the Ethereum specification \cite{wood2014yellow}. Conceptually, Ethereum maintains a globally agreed state and applies transactions that deterministically transform that state; the resulting state root is what consensus ultimately commits to. This ``programmable ledger'' framing is essential for smart contract security: vulnerabilities are not just bugs in isolated programs, but faults in state transitions that adversaries can trigger in a public, economically incentivized environment.

\subsection{Blockchain}
A blockchain is a distributed ledger maintained by a network of peer-to-peer nodes rather than a single central authority. It grows as a chain of blocks, each block holding a complete list of transaction records and containing a cryptographic link to its predecessor --- when enough subsequent blocks accumulate, reversing or altering a transaction becomes infeasible. Participants use addresses instead of real-world identities, allowing transparency while preserving pseudonymity. Because every node holds a copy of the chain and the full history of transactions is publicly verifiable, the system enables auditability: flows of value can be traced, state transitions inspected and the integrity of past operations confirmed. In short, starting from the genesis block and continuing through every appended block, the blockchain combines decentralization, tamper-resistance, and verifiable history in one architecture \cite{blockchain}.


\begin{figure}[!h]
    \centering
    \includegraphics[width=10cm, height=9cm]{images/blockchain_diagram_eth.pdf}
    \caption{Structure of an Ethereum blockchain node.}
    \label{fig:blockchain_diagram_eth}
\end{figure}

In Figure \ref{fig:blockchain_diagram_eth}, a blockchain block is shown linking to its predecessor and successor by storing the header hash of the previous block in its own header, forming an immutable chain; the header also includes the Merkle root, a timestamp and a nonce. The lower portion expands the block transaction set into a binary Merkle tree: individual transactions are hashed, then combined pairwise and re-hashed level by level until a single root hash remains. That root inserted in the block header acts as a compact commitment to every transaction in the block, so any alteration to a transaction propagates through the tree and changes the root, invalidating the header. This design allows nodes to verify the inclusion of specific transactions efficiently without reprocessing the entire block, while the previous-hash pointer secures block ordering \cite{blockchain_diagram_eth}.

\subsubsection*{Merkle Tree}
A Merkle tree (hash tree) is a binary tree in which each leaf holds the cryptographic hash of a data element and each internal node holds the hash of the concatenation of its two children. Starting from a set of items (e.g., transactions), we hash each item to form the leaves, then iteratively hash pairs of nodes until a single value remains: the \emph{Merkle root}. Because each internal label is a hash of its children, any change to any leaf propagates upward and deterministically alters the root. Under standard collision- and preimage-resistance assumptions for the hash function, the root acts as a binding, compact commitment to the entire dataset \cite{merkle1987,nakamoto2008}.

Merkle trees are used in public blockchains to commit to all transactions in a block using only the root in the block header. This design enables efficient \emph{inclusion proofs} (also called \emph{Merkle proofs}) of size $O(\log n)$ for a block with $n$ transactions: to prove that a transaction is part of a block, a verifier only needs the transaction, the header (containing the root), and the minimal set of sibling hashes along the path from the leaf to the root, rather than the entire block. Such proofs underpin ``light'' or SPV clients that validate inclusion without reprocessing all transactions, saving bandwidth and storage while preserving data integrity \cite{nakamoto2008}.
In addition to data commitments, a blockchain needs a consensus mechanism: a protocol by which distributed nodes agree which block sequence is canonical even in the presence of network delays and potentially adversarial participants. For permissionless public chains, consensus must tolerate open participation and economically motivated adversaries. Two widely deployed families are proof-of-work (PoW), which selects block producers in proportion to computational work, and proof-of-stake (PoS), which selects them in proportion to locked economic stake \cite{blockchain,consensus}.
\textit{TODO IMAGE: High-level comparison of PoW vs PoS (leader selection, finality, main attack surfaces).}

\subsection{Consensus mechanisms}
In a PoW system, network participants (miners) compete to solve computational puzzles (e.g., find a nonce so that the hash of the block header meets a target) and thereby propose the next valid block. In contrast, PoS selects block proposers (validators) based on their stake in the network (i.e., the amount of tokens locked up) rather than raw computing power, thereby altering the incentive, security and energy-profile of the system.

\subsubsection*{Proof of Work}
In PoW, miners repeatedly vary a nonce until the block header hash satisfies a difficulty target; the first miner to find a valid header can propose the next block and claim the associated reward \cite{nakamoto2008}. The expected probability of winning a block is proportional to the miner's share of total hash power, which turns consensus into an economically incentivized competition \cite{consensus}. PoW achieves security by making block production costly: rewriting history requires redoing (and overtaking) the accumulated work, which becomes infeasible unless an adversary controls a dominant fraction of network hash power \cite{nakamoto2008}. The primary downside is resource expenditure: the security mechanism is deliberately tied to large-scale computation, which translates into energy use and hardware competition \cite{consensus}.


\subsubsection*{Proof of Stake}
In PoS, validators are selected according to stake and protocol-specific randomness rather than computational power, and they can be penalized (slashed) for violating consensus rules \cite{consensus}. The key idea is to replace ``work'' with ``collateral'': attacks become expensive because they risk destroying the attacker's own stake, and honest participation is rewarded. Ethereum transitioned from PoW to PoS in ``The Merge''; for smart contract security analysis, this primarily affects block production and finality assumptions, while EVM execution remains deterministic given the state and transaction input \cite{ethereum_merge,wood2014yellow}.
With a solid understanding of how blocks are structured and how consensus enables nodes to agree on a canonical chain, we now turn to the next layer of the platform: how transactions and smart contract code are executed.

\section{Ethereum Virtual Machine}
The Ethereum Virtual Machine (EVM) is a deterministic state machine that executes the bytecode of the smart contract identically on all nodes \cite{wood2014yellow}. However, it is not Turing complete in the classical sense: execution is strictly resource-bounded by gas - every instruction has a metered gas cost, and execution halts once the supplied gas is exhausted. 
This gas-bounded execution guarantees termination and prevents non-halting or deliberately expensive programs from stalling the network; accordingly, the EVM is often described as \emph{quasi–Turing complete} (computationally universal only under a finite gas budget) \cite{antonopoulos2018mastering}\cite{wood2014yellow}.

\subsection{Gas}
Gas is the unit of account for computational effort in Ethereum. Each EVM opcode has an associated gas cost, and a transaction specifies an upper bound (gas limit) on how much gas may be consumed during its execution.

Gas serves three core purposes:
\begin{itemize}
    \item \emph{metering} computation and storage, so that the use of the resources is paid for by the initiator of a transaction.
    \item \emph{DoS resistance} and spam prevention, because excessively resource-intensive executions become economically infeasible \cite{wood2014yellow}\cite{article_1559}.
    \item \emph{economic prioritization} of limited block space via fees
\end{itemize}

Since EIP-1559, each block includes a dynamically adjusted \emph{base fee} that is burned and a user-specified \emph{priority fee} (tip) paid to the block proposer \cite{eip1559,article_1559}.
Users submit \texttt{maxFeePerGas} and \texttt{maxPriorityFeePerGas}; the effective price paid per gas equals \(\min(\texttt{maxFeePerGas}, \text{base fee} + \texttt{maxPriorityFeePerGas})\). 
Blocks have an elastic gas target and the base fee increases or decreases depending on recent block gas usage, stabilizing the fee market while preserving incentives.


\subsection{Architecture}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{images/evm}
    \caption[Ethereum Virtual Machine illustration]{Ethereum Virtual Machine illustration\footnote{\url{https://ethereum.org/developers/docs/evm/}}.}
    \label{fig:evm}
\end{figure}

Figure~\ref{fig:evm} summarizes the main components involved in a single EVM execution. The program counter (PC) selects the next opcode in the contract's bytecode; opcodes operate on the stack and can read and write volatile memory as well as persistent account storage \cite{wood2014yellow}.
The EVM is executed independently by every node, so any non-determinism would break consensus; as a result, all observable effects of execution are fully determined by the current state and the transaction input \cite{wood2014yellow}.

where
\begin{itemize}
    \item Program Counter (PC) tracks the index of the next instruction to execute.
    \item EVM bytecode is immutable code stored with the account and interpreted by the EVM.
    \item Account storage is persistent key-value state associated with a contract account and stored in the global state \cite{wood2014yellow}.
\end{itemize}

\subsection{Memory}

\begin{itemize}
    \item \textbf{Stack} is a last-in, first-out structure used for evaluation of most opcodes; it is bounded (1024 items) and word-sized (256-bit) \cite{wood2014yellow}.
    \item \textbf{Memory} is a byte-addressed, transient area that is allocated per execution and discarded after the call returns; it expands dynamically and expansion is charged in gas \cite{wood2014yellow}.
    \item \textbf{Storage} is persistent state associated with a contract account. Conceptually, it is a key-value map from 256-bit words to 256-bit words and is part of the global state \cite{wood2014yellow}.
    \item \textbf{Calldata} is a read-only byte array containing the call input (function selector and arguments) for the current message call \cite{wood2014yellow}.
\end{itemize}

\section{Smart Contracts}
% Solidity only to the extent necessary for your later vulnerability patterns + tool parsing.
\subsection{Solidity}
Ethereum smart contracts are programs whose state and code live on-chain and are executed by the EVM when triggered by transactions or internal message calls \cite{wood2014yellow}. In practice, most contracts are written in Solidity and compiled to EVM bytecode; users and other contracts interact with them through an Application Binary Interface (ABI) that specifies how to encode function calls and decode return data \cite{antonopoulos2018mastering,soliditydocs}.

Solidity is a contract-oriented language with features that are directly relevant to security analysis: inheritance and overriding, interfaces, libraries, and a rich type system including mappings and dynamic arrays \cite{soliditydocs}. The language also exposes EVM concepts such as \texttt{msg.sender}, \texttt{msg.value}, and low-level calls (\texttt{call}, \texttt{delegatecall}, \texttt{staticcall}) that influence control flow and trust boundaries. These features make it possible to implement sophisticated protocols, but they also create pitfalls: subtle differences in call semantics, unexpected reverts, and mismatches between programmer assumptions and EVM-level behavior are common sources of vulnerabilities.

\textit{TODO IMAGE: Solidity compilation and execution pipeline (Solidity source $\rightarrow$ compiler $\rightarrow$ bytecode $\rightarrow$ EVM execution/state changes).}


\section{Security}
\subsection{Vulnerability Taxonomies}
Smart contract security research and industry practice have converged on a set of recurring vulnerability categories. Two widely used resources are the Smart Contract Weakness Classification (SWC) registry and the OWASP Smart Contract Top 10, both of which provide structured names, short descriptions, and representative examples for common classes of issues \cite{swc_registry,owasp_sc_top10}. Using a shared taxonomy is not merely a reporting convenience: it enables consistent labeling across audit reports, static analyzers, and vulnerability datasets, and it makes retrieval-based systems (such as the knowledge base used later in this thesis) easier to query and evaluate.

That said, taxonomies are imperfect abstractions. Many real incidents do not map cleanly to a single label: an exploit may combine oracle manipulation with reentrancy, or rely on an access control mistake that only becomes exploitable due to an accounting bug. For this reason, we treat taxonomies as an organizational tool rather than a complete explanation of risk, and we emphasize concrete vulnerability \emph{patterns} that are actionable during analysis \cite{atzei2017survey,perez2021smart}.
\subsection{Vulnerability Patterns (Practical)}
% Your “working set” of patterns referenced later by the KB.
\subsubsection*{Reentrancy and External Call Hazards}
\emph{Reentrancy} occurs when a contract makes an external call before it has fully updated its own state, allowing the callee to re-enter and observe or manipulate intermediate state \cite{luu2016making,atzei2017survey}. In practice, the underlying hazard is broader than classic reentrancy: any external call can revert unexpectedly, consume unbounded gas, or execute arbitrary code in the callee, so call ordering and error handling are security-critical \cite{consensys_best_practices}. A common mitigation is the checks-effects-interactions pattern, complemented by explicit reentrancy guards where appropriate \cite{consensys_best_practices}.
\subsubsection*{Access Control and Authorization Bugs}
\emph{Access control} issues arise when privileged operations are callable by unintended parties, or when authorization checks are missing, inconsistent, or based on manipulable state \cite{atzei2017survey,consensys_best_practices}. Typical examples include unprotected administrative functions, incorrect role checks, and unsafe assumptions about \texttt{msg.sender} in proxy and meta-transaction settings. Because Solidity and the EVM do not provide built-in role management, secure authorization is largely a matter of correct application logic and disciplined use of vetted libraries \cite{consensys_best_practices}.
\subsubsection*{Arithmetic / Precision / Accounting Errors}
\emph{Arithmetic and accounting} bugs include rounding errors, inconsistent unit conventions, and flawed share-to-asset conversions that break conservation properties (e.g., ``users cannot withdraw more than their share'') \cite{atzei2017survey,perez2021smart}. Even when basic integer overflows are checked by the language or libraries, protocols remain vulnerable to precision loss in division, time-weighted calculations, or incorrect handling of token decimals. These bugs are particularly dangerous in DeFi because they often create small, repeatable extraction opportunities that accumulate into large losses.
\subsubsection*{DoS and Gas Griefing}
\emph{Denial of service} in Ethereum often takes the form of making a critical operation prohibitively expensive or consistently reverting. Examples include iterating over unbounded lists, relying on external calls that can be forced to revert, or designs where a single malicious participant can block progress by refusing to cooperate \cite{atzei2017survey}. Because gas imposes a hard resource budget, some failures are not ``bugs'' in the conventional sense but brittleness in protocol design under adversarial conditions. Defensive patterns include bounded iteration, pull-based payments, and explicit fail-open/fail-closed decisions with documented assumptions \cite{consensys_best_practices}.
\subsubsection*{Front-running / MEV-related Issues (when relevant)}
Ethereum transactions are visible in the mempool before inclusion, and block producers can influence ordering within a block. This creates opportunities for \emph{front-running} and broader Maximal Extractable Value (MEV) strategies, where adversaries profit from reordering or inserting transactions around a victim transaction \cite{daian2020flash}. In protocol design, this shows up as price manipulation during trades, sandwich attacks, and liquidation races. Mitigations depend heavily on the application and often require mechanism design choices (e.g., commit-reveal, batch auctions, or oracle design) rather than purely local code changes \cite{daian2020flash}.
\subsubsection*{Upgradeability and Initialization Bugs}
\emph{Upgradeability} introduces a second-order attack surface: even if the implementation code is correct, insecure upgrade authorization or incorrect initialization can make the system trivially compromisable \cite{openzeppelin_upgrades}. Common pitfalls include uninitialized proxy implementations, re-initialization bugs, and storage layout collisions across upgrades. Because upgrades often interact with governance and multisig procedures, audits must treat the upgrade path as part of the trusted computing base, not as an auxiliary feature \cite{openzeppelin_upgrades}.
% For each: 2–5 sentences + 1 tiny example later in Implementation/Eval if needed.

\section{Audit Methodology}

Smart contract auditing has evolved into a specialized discipline that combines elements of traditional software security review with domain-specific knowledge about blockchain execution and economic attack vectors. Unlike conventional software where bugs might cause crashes or data corruption, smart contract vulnerabilities can lead directly to irreversible financial loss because the adversary can execute the exploit path as soon as it becomes profitable and the code is difficult to patch after deployment \cite{perez2021smart,atzei2017survey}.

Auditors begin by establishing what the system is supposed to do and identifying potential threat vectors, then systematically examine the implementation through a combination of manual review and automated tools. While human expertise remains central to catching subtle logic bugs and business logic flaws, automation has become increasingly important for handling the scale and complexity of modern DeFi protocols \cite{perez2021smart}. 

\subsubsection*{System overview}
An audit typically produces (i) a threat model and list of assumptions, (ii) a set of findings with severity and exploit scenarios, and (iii) concrete remediation guidance that developers can implement and reviewers can re-check. Although practices vary across teams, a common workflow iterates between high-level reasoning (protocol intent, assets at risk, trust boundaries) and low-level validation (line-by-line review, tool outputs, and test results). In later chapters, we will mirror this workflow in an automated setting by combining precise program analysis with an LLM that can retrieve relevant context and explain findings in human-oriented language.
\textit{TODO IMAGE: Audit workflow diagram (spec/threat model $\rightarrow$ manual review $\rightarrow$ static tools $\rightarrow$ testing/fuzzing $\rightarrow$ reporting and remediation).}

\subsubsection*{Trust model}

Before examining a single line of code, effective audits begin with understanding what the system is designed to do and what could go wrong. This specification and threat modeling phase establishes the foundation for all subsequent analysis. Auditors work with the development team to document intended behavior, including state invariants that should always hold, access control assumptions about who can perform which actions, and economic mechanisms that should remain balanced under adversarial conditions \cite{perez2021smart,consensys_best_practices}.

State invariants are particularly critical in DeFi protocols. For example, an automated market maker (AMM) might have an invariant that the product of token reserves remains constant except during fee accrual, or a lending protocol might require that total collateral always exceeds total debt by some minimum margin. Documenting these invariants explicitly serves multiple purposes: it guides manual review by clarifying what properties to verify, it provides targets for property-based fuzzing tools, and it establishes success criteria for testing. When invariants are violated, it often indicates either a vulnerability in the code or a gap in the original specification that needs addressing.

Trust assumptions define the threat model—which actors are assumed to be honest, which might be malicious, and what capabilities each has. A typical DeFi protocol might trust its governance multi-sig to upgrade contracts responsibly while assuming that all regular users will act to maximize their own economic benefit, including front-running transactions and exploiting any profitable vulnerabilities. External dependencies also factor into the trust model: does the protocol rely on specific oracle providers, and what happens if those oracles are compromised or manipulated? These assumptions directly influence what the auditor looks for in the code.

This phase also identifies high-value targets—the functions and state variables most critical to protocol security and most likely to be attacked. In a lending protocol, this might be functions that calculate collateralization ratios or trigger liquidations. In a bridge, it could be the validation logic for cross-chain messages. By explicitly mapping out these critical components and their security requirements up front, auditors can allocate their limited time more effectively and ensure comprehensive coverage of the attack surface. This specification work also sets up what an LLM-based agent should retrieve when analyzing code: knowing the declared invariants and trust assumptions allows the agent to focus its limited context window on retrieving code segments most relevant to verifying those properties.

\subsubsection{Manual Code Review}

Manual code review remains the cornerstone of smart contract auditing despite advances in automation. Experienced auditors develop intuition about common vulnerability patterns, but more importantly, they can reason about complex interactions, business logic edge cases, and economic attack vectors that current tools struggle to identify. The review process typically proceeds through several phases, each with a different focus and level of detail.

The review begins with architectural analysis—understanding how contracts interact, what external systems they depend on, and how data flows through the protocol. Auditors construct mental models (and often diagrams) of the system's components and their relationships. For example, in a yield aggregation protocol, they map out how user deposits flow into various strategies, how yield is calculated and distributed, and what happens during rebalancing or emergency withdrawals. This high-level understanding helps identify architectural vulnerabilities like dangerous centralization points, circular dependencies that could create reentrancy opportunities, or reliance on external systems that could fail.

With the architecture clear, auditors move to examining critical execution paths. They trace through key user interactions step by step: what happens when Alice deposits tokens, when Bob triggers a liquidation, when governance proposes a parameter change? This path analysis helps identify missing checks, incorrect sequencing, or edge cases where the code behaves unexpectedly. Auditors pay special attention to paths involving external calls, token transfers, and state changes—the operations most likely to introduce vulnerabilities.

Invariant verification forms another crucial part of manual review. Armed with the invariants documented during threat modeling, auditors examine whether the code actually maintains these properties under all possible executions. This often involves reasoning about potential sequences of function calls and state transitions. For instance, if a protocol claims that "users can always withdraw their proportional share of pooled assets," the auditor checks whether there are any code paths where this could fail—perhaps during contract upgrades, when certain functions are paused, or in response to specific market conditions.

Edge case analysis requires creativity and experience. Auditors consider boundary conditions: what happens with zero values, maximum values, or just-below-overflow values? They think about timing issues: what if a transaction is front-run, what if the block timestamp is manipulated slightly, what if multiple operations happen in the same block? They examine error handling: are all failure modes properly addressed, or are there paths where errors could leave the system in an inconsistent state? This type of reasoning is difficult to automate because it requires understanding both the code's literal behavior and the protocol's semantic intent.

The manual review process is inherently limited by human cognitive constraints. An auditor can only hold so much context in working memory at once, and complex protocols might span dozens of contracts with thousands of lines of code. This is where tool-assisted workflows become valuable—not to replace human judgment, but to help auditors manage complexity by automating context retrieval, flagging potential issues for human investigation, and maintaining consistency in coverage across a large codebase. The goal of integrating LLMs into this workflow is to augment human auditors with better information retrieval and pattern recognition, allowing them to focus their expertise on the creative reasoning tasks that machines still cannot perform.

\subsubsection*{Tool-Based Analysis}

Automated analysis tools have become an essential part of the smart contract audit workflow, complementing manual review by efficiently checking for known vulnerability patterns and coding errors. These tools vary in their approaches—from lightweight pattern matching to sophisticated symbolic execution—and each brings different strengths and limitations to the audit process.

Static analysis tools examine code without executing it, using techniques ranging from simple syntactic pattern matching to complex abstract interpretation. Slither, developed by Trail of Bits, has emerged as one of the most widely adopted static analyzers for Solidity \cite{feist2019slither}. It operates on Solidity's intermediate representation (IR) and includes over 70 built-in detectors for common issues like reentrancy vulnerabilities, unprotected ether withdrawals, incorrect ERC-20 implementations, and dangerous delegatecalls. Slither's strength lies in its speed and comprehensive coverage—it can analyze a medium-sized project in seconds and rarely produces false negatives for the patterns it targets.

However, static analyzers face fundamental limitations. They must over-approximate program behavior to be sound, which often leads to false positives—flagging code as potentially vulnerable when it's actually safe. For example, Slither might flag a reentrancy warning for any external call followed by a state change, even when the specific call cannot actually re-enter the contract. Auditors must manually review these warnings to separate real issues from noise. More critically, pattern-based detectors can only find vulnerabilities they were explicitly programmed to recognize. Novel vulnerability classes or complex logic bugs that don't match known patterns slip through entirely.

Other static analysis tools take different approaches. Mythril employs symbolic execution and SMT solving to explore possible execution paths and find states that violate security properties \cite{mythril}. This can uncover deeper issues than pattern matching, but symbolic execution doesn't scale well to large contracts or complex path conditions—the number of possible paths grows exponentially with program size and branching. Securify uses a dataflow analysis framework and security patterns specified in a domain-specific language \cite{tsankov2018securify}, offering more flexibility in specifying what to look for but requiring expertise to write effective patterns.

Linters like Solhint and Ethlint catch style violations and potential code quality issues rather than security vulnerabilities per se. While less critical than security-focused analyzers, they help maintain code quality that makes contracts easier to audit. They flag issues like missing visibility specifiers, unused variables, overly complex functions, and deviations from best practice patterns like checks-effects-interactions.

Integration of multiple tools provides better coverage than relying on any single analyzer. A typical audit workflow might run Slither for quick pattern detection, Mythril for deeper analysis of critical functions, and various linters for code quality checks. However, this multi-tool approach creates new challenges: each tool has its own output format, overlapping detections need deduplication, and the auditor must triage dozens or hundreds of findings to identify which require attention.

This is where the Wake framework becomes relevant to our work. Wake provides a Python-based infrastructure for building custom detectors and integrating various analysis tools in a unified workflow \cite{wake2023documentation}. Unlike standalone tools that operate in isolation, Wake allows auditors to write detectors that combine insights from multiple sources—for example, using both AST pattern matching and data flow analysis to reduce false positives. Wake's Python ecosystem also makes it a natural bridge to LLM-based analysis: we can extract rich context from Wake's analysis passes and feed it to language models that can reason about patterns too complex for rule-based systems.

The key insight for our work is that automated tools excel at scalable, repeatable pattern matching but struggle with novel vulnerabilities and contextual reasoning. Meanwhile, LLMs demonstrate strong pattern recognition on code they've seen during training and can generate natural language explanations, but they lack the precision and soundness guarantees of formal methods. By combining static analysis tools with LLM reasoning in an agent-based architecture—using tools like Wake to extract structured information and RAG to provide relevant context—we aim to get closer to the best of both approaches.

\subsubsection*{Local deployment}

Testing smart contracts in realistic conditions requires deploying them to a local blockchain environment where their behavior can be observed without risking real funds or incurring transaction costs. Modern development frameworks like Hardhat, Foundry, and Wake provide robust local testing infrastructure that has become standard in the audit workflow \cite{hardhat2023, foundry2023}.

Unit tests verify individual functions in isolation, checking that they produce expected outputs for given inputs and properly handle error conditions. Integration tests examine how multiple contracts interact, ensuring that complex workflows execute correctly end-to-end. For example, testing a lending protocol might involve simulating a sequence of deposits, borrows, interest accrual, and liquidations to verify that all state transitions occur correctly and invariants are maintained. Well-written test suites serve both as verification of correct behavior and as documentation of intended functionality.

Fork testing represents a particularly powerful technique for smart contract auditing. Rather than testing in isolation, fork testing deploys the contract under audit to a local blockchain that has copied the state of mainnet at a specific block. This allows the contract to interact with real deployed protocols—actual DEXes, oracles, tokens—without requiring the auditor to mock these complex systems. For example, when auditing a yield optimization strategy, fork testing can verify that the strategy actually generates yield when deployed against real DeFi protocols, that it handles real market conditions correctly, and that it responds appropriately to real oracle updates.

Fork testing also enables reproduction and investigation of historical exploits. By forking at a block just before a known attack, auditors can replay the attack transaction to understand exactly how it worked, then test whether proposed fixes would have prevented it. This forensic capability helps validate that audit recommendations actually address the vulnerabilities they target. Wake's testing framework includes sophisticated fork testing capabilities, allowing tests to manipulate fork state, impersonate arbitrary accounts, and observe detailed execution traces.

Despite their value, tests have inherent limitations. Test coverage is only as good as the test cases written—they can demonstrate the presence of bugs but never prove their absence. Auditors who rely too heavily on "the tests pass" as evidence of security may miss edge cases that weren't considered when writing tests. Additionally, tests typically focus on expected usage patterns, while attackers deliberately seek unexpected input combinations and state transitions that developers didn't anticipate.

This limitation motivates property-based testing approaches, where instead of writing individual test cases, auditors specify properties that should always hold and let the testing framework automatically generate inputs trying to violate those properties. This bridges to our next topic: fuzz testing, which takes this idea even further by using randomization and evolutionary algorithms to explore the state space more thoroughly than human-written tests ever could.

\subsubsection*{Fuzz testing}

Fuzzing has emerged as one of the most effective techniques for uncovering subtle bugs in smart contracts, particularly logic errors that evade static analysis and aren't covered by manual test cases. Unlike traditional testing where humans specify input values, fuzzing automatically generates large numbers of random or semi-random inputs to explore program behavior across a wide range of scenarios \cite{manes2019art,grieco2020echidna}.

The simplest form of fuzzing, random input generation, feeds functions with arbitrary values and checks whether they crash, revert unexpectedly, or violate assertions. While naive, this approach can quickly find boundary condition bugs—what happens with zero values, maximum uint256 values, or unexpected combinations? More sophisticated fuzzers use coverage-guided generation, tracking which code paths have been executed and favoring inputs that reach new branches. This evolutionary approach efficiently explores the state space, focusing computational effort on finding inputs that trigger novel behaviors.

Property-based fuzzing, exemplified by tools like Echidna \cite{grieco2020echidna} and Foundry's fuzzer \cite{foundry2023}, takes this further by allowing auditors to specify invariants that should always hold. The fuzzer then attempts to find any sequence of function calls and inputs that violates these invariants. For example, an auditor might specify that "total shares times share price should equal total assets" for a vault contract. The fuzzer will generate thousands of random deposit, withdraw, and transfer operations, checking after each whether this invariant still holds. When it finds a violation, it automatically minimizes the test case to the simplest sequence that reproduces the issue.

Stateful fuzzing considers sequences of operations rather than isolated function calls. Many vulnerabilities only emerge through specific state transitions—for example, a reentrancy bug that requires calling function A to set up state, then reentering during a call to function B to exploit that state. Echidna maintains a model of contract state and generates sequences of transactions that explore different state transition paths. This catches bugs that unit tests miss because the test writer didn't consider that particular sequence of operations.

Fuzzing excels at finding logic bugs—violations of business rules or implicit assumptions that static analyzers can't detect because they aren't simple pattern matches. For instance, fuzzing might discover that a specific sequence of deposits and withdrawals allows a user to extract more funds than they deposited, or that under certain market conditions a pricing oracle can be manipulated. These are precisely the high-impact bugs that make auditing challenging: they're correct in terms of type safety and basic invariants, but incorrect in terms of the protocol's economic logic.

However, fuzzing has limitations. It's probabilistic rather than exhaustive—finding a bug depends on the fuzzer happening to generate the right sequence of inputs, which might be a tiny fraction of the total input space. Complex bugs requiring specific state setups might be missed if the fuzzer doesn't happen to explore that path. Additionally, writing good invariant specifications requires expertise—if the auditor doesn't correctly specify what should always be true, the fuzzer won't detect violations.

The integration of fuzzing into the audit workflow has become standard practice. Auditors typically run fuzzers overnight or over weekends, generating millions of test cases to explore state spaces too large for manual testing. When combined with static analysis (which finds known patterns quickly) and manual review (which applies human reasoning about business logic), fuzzing provides a powerful third pillar in the defense against vulnerabilities. For our work with LLM-based agents, fuzzing results provide valuable signal: when a fuzzer finds an invariant violation, the LLM can analyze the failing sequence to explain why it violates the invariant and suggest fixes.

Rather than viewing automation as competing with human auditors, we see it as extending their capabilities. The techniques we've discussed generate vast amounts of information: static analyzers flag hundreds of potential issues, tests exercise thousands of state transitions, fuzzers generate millions of inputs. Making effective use of this information requires systems that can retrieve relevant context, synthesize insights across tools, and present findings in ways that augment human reasoning.

The code analysis approaches we examine next build on the audit methodology foundations established here. We explore how static analysis frameworks like Wake provide structured access to code properties, how retrieval systems can surface relevant code segments for LLM analysis, and how agent architectures can orchestrate multiple tools to tackle complex analysis tasks. The goal is not to replace the auditor workflows described in this chapter, but to make them more efficient and effective by automating the retrieval and synthesis steps that currently consume significant auditor time.

\chapter{Code Analysis}
\label{ch:code-analysis}

% Goal: describe analysis methods you'll reference when positioning Wake/Wake-AI and your RAG design.

Security analysis tools are ultimately program analysis tools: they extract facts about program behaviour and then check those facts against security expectations. In the smart contract setting, this analysis is shaped by a public, adversarial execution environment and by heavy code reuse across libraries and protocols. As a result, practical vulnerabilities often depend on cross-function and cross-contract behaviour rather than on a single isolated statement.

This chapter reviews the analysis techniques that later chapters rely on when integrating Wake with LLM-based assistance. We organise the discussion around three complementary paradigms. \emph{Static analysis} reasons about potential executions without running the program; it scales well but typically over-approximates. \emph{Dynamic analysis} observes concrete executions; it is precise for the executions it covers but cannot guarantee the absence of bugs. \emph{Formal verification} attempts to prove properties with mathematical certainty, but it requires explicit specifications and often substantial engineering effort \cite{nielson1999principles,clarke2018model}.
\textit{TODO IMAGE: Overview of analysis techniques and their typical artifacts (AST/IR/CFG/DDG, traces, proofs) and tradeoffs (scalability vs precision vs assurance).}

\section{Static Analysis}
\label{sec:static-analysis}

Static analysis encompasses techniques that examine source code, bytecode, or other program representations to infer properties about program behaviour without executing the program \cite{nielson1999principles}. Many non-trivial semantic properties are undecidable in general, so practical analyzers rely on approximations \cite{rice1953classes}. A common design goal is \emph{soundness} with respect to a targeted bug class: the analysis over-approximates possible behaviours to reduce false negatives, but it may produce false positives that require human triage \cite{nielson1999principles}.

In the context of smart contract security, static analysis has become the predominant first line of defence. Tools such as Slither~\cite{feist2019slither}, Securify~\cite{tsankov2018securify}, and Mythril~\cite{mythril} employ various static techniques to detect common vulnerability patterns including reentrancy, integer overflow, and access control violations. The effectiveness of these tools depends critically on the program representations they employ and the algorithms they execute over those representations.

\subsection{Abstract Syntax Tree}
\label{subsec:ast}

The abstract syntax tree (AST) is a hierarchical representation of source code that preserves syntactic structure while omitting concrete details that do not affect parsing \cite{aho2006compilers}. Each node corresponds to a language construct—such as a function declaration, assignment, or expression—and the tree structure reflects nesting and composition.

For Solidity smart contracts, the AST provides a natural representation for pattern matching and feature extraction. Consider the detection of unchecked external calls, a prerequisite for reentrancy vulnerabilities. An analyser can traverse the AST to identify \texttt{call} expressions, examine their surrounding context to determine whether return values are checked, and flag instances where failures are silently ignored. This pattern-based approach, while conceptually simple, proves remarkably effective for detecting many common vulnerability classes~\cite{feist2019slither}.

The AST also serves as the basis for computing syntactic metrics and extracting features for machine learning approaches. Metrics such as cyclomatic complexity, function length, and nesting depth can be computed directly from AST traversal~\cite{mccabe1976complexity}. More sophisticated feature extraction might identify the presence of specific syntactic patterns—loops containing external calls, state modifications after external calls, or complex inheritance hierarchies—that correlate with vulnerability presence.

However, the AST has significant limitations for semantic analysis. Because it captures only syntactic structure, it cannot directly represent the flow of data through a program or the possible paths of control flow. Two syntactically similar code fragments may exhibit vastly different runtime behaviours depending on the values of variables and the paths taken through conditional statements. These limitations motivate the use of more semantically rich representations.

\subsection{Intermediate Representation}
\label{subsec:ir}

Intermediate representations (IRs) bridge the gap between source-level syntax and execution semantics by normalising diverse syntactic constructs into a more uniform format \cite{muchnick1997advanced}. A well-designed IR reduces syntactic variability, makes implicit operations explicit, and exposes structure that simplifies later analyses.

In the context of Solidity analysis, IRs play a crucial role in normalising the language's many idiosyncrasies. Solidity supports multiple ways to perform external calls (\texttt{call}, \texttt{delegatecall}, \texttt{transfer}, \texttt{send}), various forms of type coercion, and complex inheritance semantics with function overriding. An IR can normalise these constructs into a smaller set of primitive operations, simplifying subsequent analysis~\cite{grech2018madmax}.

The Ethereum Virtual Machine (EVM) bytecode itself can be analyzed directly, but it presents significant challenges. EVM bytecode uses a stack-based execution model and allows dynamically computed jump destinations, which makes reliable control-flow recovery difficult \cite{brent2018vandal}. Higher-level IRs that retain source-level information while simplifying semantics (including compiler IRs such as Yul) are often more tractable analysis targets.

Wake's internal model employs a custom IR that preserves source mapping information while normalising Solidity constructs for analysis. This design choice enables Wake to perform sophisticated cross-function analysis while maintaining the ability to report findings at the source level, a crucial requirement for actionable security reports. The IR serves as the foundation upon which control flow and data flow analyses are constructed.

\subsection{Control Flow Analysis}
\label{subsec:control-flow}

Control flow analysis (CFA) determines the possible paths of execution through a program. The standard artifact is a control flow graph (CFG), where nodes represent basic blocks—sequences of instructions with no internal branching—and edges represent possible transfers of control \cite{aho2006compilers,nielson1999principles}. The CFG provides the structural backbone for many subsequent analyses by enabling reasoning about reachability, loop structure, and the ordering of operations.

For smart contract security, control flow analysis enables the detection of vulnerabilities that depend on execution ordering. Reentrancy vulnerabilities, for example, occur when an external call can transfer control to an attacker who then re-enters the vulnerable contract before the original invocation completes. Detecting this pattern requires understanding that certain paths through the CFG place external calls before state updates, violating the checks-effects-interactions pattern~\cite{consensys_best_practices}.

The call graph captures calling relationships between functions and extends control-flow reasoning to the interprocedural level. Each node represents a function, and edges represent potential call relationships. Call graph construction becomes challenging in the presence of dynamic dispatch and late binding, which occur in object-oriented systems and appear in Solidity via inheritance and overriding \cite{grove2001framework}.

For Solidity, the call graph must account for both internal function calls and external message calls to other contracts. External calls introduce additional complexity because the target contract's code may not be available for analysis, and even when available, the call may be dispatched through a proxy pattern that obscures the actual implementation~\cite{chen2020defining}. Sound analysis requires conservative approximation of possible call targets, potentially leading to precision loss.

Reachability analysis over the CFG determines which program points can potentially be reached from a given starting point. This analysis is fundamental for pruning infeasible paths from consideration and for establishing preconditions under which vulnerabilities can be triggered. If a vulnerable code pattern exists only on unreachable paths, it poses no actual security risk.

\subsection{Data Flow Analysis}
\label{subsec:data-flow}

Data flow analysis tracks how values flow through a program, determining at each program point which values may be produced and consumed~\cite{nielson1999principles}. The classic formulation computes fixed points over a lattice of data-flow facts by iteratively propagating information along CFG edges until no further changes occur~\cite{nielson1999principles}.

The reaching definitions analysis, a foundational data flow analysis, determines for each use of a variable which definitions (assignments) might reach that use. This information enables detection of uninitialised variables, dead code, and various forms of data dependency~\cite{nielson1999principles}. For smart contract security, reaching definitions help establish whether user-controlled input can influence security-critical operations.

Taint analysis, a specialisation of data flow analysis, tracks the flow of untrusted data from sources (such as transaction parameters) to security-sensitive sinks (such as external calls or state modifications)~\cite{schwartz2010all}. In the smart contract domain, taint sources include \texttt{msg.sender}, \texttt{msg.value}, \texttt{calldata}, and return values from external calls. Sinks include operations that transfer value, modify access control state, or perform external calls. A taint flow from source to sink without proper sanitisation indicates a potential vulnerability.

Data flow analysis faces particular challenges in Solidity due to storage aliasing. Solidity's storage model maps state variables to 256-bit storage slots according to complex packing rules, and dynamic data structures such as mappings and arrays use hash-based slot computation~\cite{antonopoulos2018mastering}. Two syntactically distinct storage references may alias to the same slot, creating data flow paths invisible to na\"ive analysis. Precise handling of storage aliasing requires modelling Solidity's storage layout rules and potentially employing alias analysis techniques~\cite{andersen1994program}.

Additionally, the transactional semantics of smart contracts—where each transaction executes atomically and state persists between transactions—creates data flow paths that span multiple invocations. A value written in one transaction may be read in a subsequent transaction, creating cross-transaction data dependencies that single-invocation analysis cannot capture.

\subsection{Inter-procedural Analysis}
\label{subsec:inter-procedural}

While intraprocedural analysis examines individual functions in isolation, many vulnerability patterns require reasoning across function boundaries. Interprocedural analysis extends data flow and control flow analyses to operate over entire programs, tracking how values and control flow cross function boundaries~\cite{nielson1999principles}.

The fundamental challenge of interprocedural analysis is maintaining both precision and scalability. A fully context-sensitive analysis that distinguishes between different call sites can provide precise results but scales poorly, as the number of contexts may grow exponentially with call depth. Context-insensitive analysis, which merges information from all call sites, scales better but may produce imprecise results that manifest as false positives~\cite{nielson1999principles}.

Function summaries provide a middle ground, capturing the input-output behaviour of functions in a form that can be instantiated at call sites without re-analysing the function body~\cite{nielson1999principles}. For security analysis, a summary might capture which parameters can influence which return values, which state variables are read or written, and which external calls may be performed. Pre-computed summaries enable modular analysis where each function is analysed once, and the summaries are composed to reason about whole-program behaviour.

In the smart contract domain, interprocedural analysis is essential for detecting vulnerabilities that span multiple functions. Access control vulnerabilities often manifest as public functions that, through a chain of internal calls, perform privileged operations without adequate authorisation checks. Detecting such vulnerabilities requires tracing control and data flow from public entry points through internal function calls to security-sensitive operations~\cite{brent2020ethainter}.

Cross-contract analysis presents additional challenges. Smart contracts frequently interact with other contracts, and vulnerabilities may arise from the composition of individually secure contracts. Analysis of cross-contract interactions requires either whole-system analysis—which may be impossible when contract source is unavailable—or the use of conservative assumptions about external contract behaviour~\cite{grech2018madmax}.

\textit{TODO IMAGE: Example artifacts for static analysis (CFG and a small data-dependency/taint graph) for a Solidity function that makes an external call.}


\section{Dynamic Analysis}
\label{sec:dynamic-analysis}

While static analysis approximates potential executions, dynamic analysis observes actual program behaviour during concrete executions \cite{nielson1999principles}. The tradeoff is complementary: dynamic techniques provide high precision on the executions they cover, but the absence of an observed failure is not evidence that no failing execution exists.

Dynamic analysis proves particularly valuable for smart contract security because the blockchain environment involves complex interactions with external state, other contracts, and economic mechanisms that are difficult to model statically. Observing actual transaction execution can reveal behaviours that static analysis either misses or conservatively overapproximates.

\subsection{Transaction Trace Analysis}
\label{subsec:transaction-traces}

Transaction traces provide a detailed record of smart contract execution, capturing the call structure and step-by-step effects of a transaction. On Ethereum and compatible networks, traces can be obtained via node-provided tracing APIs (commonly exposed through JSON-RPC namespaces such as \texttt{debug}), enabling retrospective debugging and incident forensics \cite{geth_debug_rpc}.

A transaction trace typically includes the sequence of internal and external calls, the input data and return values for each call, state changes (storage writes), event emissions, and any revert conditions encountered. This information enables reconstruction of the precise execution path taken, facilitating debugging and forensic analysis of security incidents.

State differentials, computed by comparing contract state before and after transaction execution, reveal the net effect of a transaction on persistent storage. For security analysis, state differentials help identify transactions that modify access control state, transfer ownership, or manipulate balances in unexpected ways. Anomaly detection techniques can identify transactions whose state differentials deviate significantly from historical patterns, potentially indicating exploitation. [CITATION NEEDED]

Revert reason analysis examines transactions that failed, extracting information about the cause of failure from revert data such as error strings and custom errors \cite{soliditydocs}. Patterns in reverted transactions can reveal attempted attacks that were blocked by existing checks, informing defensive analysis. A sudden spike in reverts from a particular function might indicate an ongoing attack probe.

\subsection{Fuzz Testing}
\label{subsec:fuzz-testing}

Fuzz testing (fuzzing) is a dynamic testing technique that automatically generates test inputs to explore program behaviour and discover bugs \cite{miller1990empirical}. Modern fuzzing extends early random testing by incorporating feedback such as code coverage and by using domain-specific generators, which improves the probability of reaching deep program states \cite{manes2019art}.

For smart contract security, fuzzing must account for the stateful nature of contract execution. A single transaction rarely suffices to reach deep contract states; rather, sequences of transactions are required to manipulate contract state into configurations where vulnerabilities manifest. Stateful fuzzing maintains contract state across transaction sequences, enabling exploration of states reachable only through specific transaction orderings~\cite{grieco2020echidna}.

Invariant-based fuzzing provides properties that should hold across all reachable states. The fuzzer then attempts to generate transaction sequences that violate these invariants. Common invariants include balance conservation (total value in should equal total value out), access control consistency (only authorised addresses can perform privileged operations), and state machine integrity (the contract should only be in valid states)~\cite{grieco2020echidna,foundry2023}.

Coverage-guided fuzzing uses feedback from previous executions to guide input generation toward unexplored program regions. By instrumenting the contract to track which code paths are exercised, the fuzzer can prioritise inputs that reach new coverage, systematically expanding the explored state space~\cite{nguyen2020sfuzz}. The combination of coverage guidance with property-based testing provides a powerful technique for discovering subtle vulnerabilities that escape both manual review and pattern-based static analysis.

Corpus generation and management play crucial roles in fuzzing effectiveness. The corpus—the set of seed inputs from which the fuzzer generates new inputs—significantly influences exploration efficiency. Techniques such as corpus distillation (removing redundant inputs) and scheduling (prioritising seeds likely to lead to new coverage) improve throughput and reduce wasted execution \cite{rebert2014optimizing}.

Wake includes an integrated fuzzing framework that supports stateful property-based testing of Solidity contracts. By providing a Python-based test harness with coverage instrumentation and property specification capabilities, Wake enables developers and auditors to systematically explore contract behaviour. This dynamic analysis capability complements Wake's static analysis features, providing a more comprehensive security assessment.


\section{Formal Verification}
\label{sec:formal-verification}

Formal verification employs mathematical techniques to prove that programs satisfy specified properties within a formal model \cite{clarke2018model}. In contrast to testing, which samples executions, formal methods attempt to reason about all executions in the model. If verification succeeds, the property holds under the stated assumptions; if it fails, the method typically yields a counterexample that demonstrates a violation.

The appeal of formal verification for smart contract security is clear: contracts may manage significant financial value and are hard to patch after deployment, so strong assurance arguments are valuable \cite{atzei2017survey}. Formal verification offers the prospect of eliminating entire classes of bugs when suitable specifications are available.

However, formal verification faces significant practical challenges. The techniques are computationally expensive, often requiring substantial manual effort to specify properties and guide the verification process. Scalability limitations restrict application to smaller codebases or require abstraction that may introduce imprecision. Additionally, verification establishes conformance to specification, but specifications themselves may be incomplete or incorrect, leaving gaps in security coverage.

\subsection{Symbolic Execution}
\label{subsec:symbolic-execution}

Symbolic execution executes programs with symbolic rather than concrete values, representing program state as logical constraints over symbolic variables~\cite{king1976symbolic}. Rather than computing specific outputs for specific inputs, symbolic execution computes expressions over symbolic inputs that characterise the relationship between inputs and outputs. At branch points, execution forks to explore both paths, accumulating path conditions that constrain the symbolic inputs consistent with each path.

For vulnerability detection, symbolic execution enables systematic exploration of program paths and automatic generation of inputs that trigger specific behaviours. Given a property to check—such as the absence of integer overflow—symbolic execution explores paths, collects constraints under which the property is violated, and queries a satisfiability modulo theories (SMT) solver to determine whether satisfying inputs exist. If the solver finds a satisfying assignment, it provides a concrete exploit input; if the constraints are unsatisfiable, the property holds on that path~\cite{cadar2008exe}.

Symbolic execution faces the path explosion problem: the number of paths through a program grows exponentially with the number of branch points. Loops and recursive calls exacerbate this problem, potentially creating infinite paths. Practical symbolic execution engines employ various strategies to manage path explosion, including bounded exploration, state merging, and prioritised search~\cite{cadar2013symbolic}.

In the smart contract domain, tools such as Mythril~\cite{mythril} and Manticore~\cite{mossberg2019manticore} employ symbolic execution to detect vulnerabilities. These tools symbolically execute EVM bytecode, modelling the Ethereum execution environment including storage, message calls, and environmental variables. When a path reaches a potentially vulnerable state—an external call with user-controlled destination, an arithmetic operation that may overflow—the tool checks whether the path is feasible and, if so, reports the vulnerability with a triggering input.

Environment modelling presents particular challenges for smart contract symbolic execution. The blockchain environment includes not just the contract under analysis but the entire ecosystem of other contracts, the block context, and the transaction parameters. Accurate modelling requires assumptions about what external contracts might do, what values environmental variables might take, and how transactions might be sequenced. Conservative assumptions lead to false positives; optimistic assumptions may miss real vulnerabilities~\cite{luu2016making}.

\subsection{Model Checking}
\label{subsec:model-checking}

Model checking exhaustively explores the state space of a finite-state system to verify that specified properties hold in all reachable states~\cite{clarke1999model}. Given a model of the system and a property expressed in temporal logic, a model checker either confirms that the property holds or produces a counterexample trace demonstrating a violation.

For smart contracts, model checking can verify properties that span multiple transactions and involve temporal ordering. Properties such as ``whenever a withdrawal is requested, funds are eventually transferred'' or ``the contract never enters a state from which funds cannot be recovered'' require reasoning about sequences of states, which temporal logics naturally express~\cite{mavridou2018designing}.

The state explosion problem—the exponential growth of state space with the number of state variables—limits direct model checking to relatively small systems. Abstraction techniques address this limitation by constructing simplified models that preserve the property of interest while reducing state space \cite{clarke2018model}.

Bounded model checking (BMC) offers a pragmatic compromise, exploring executions up to a fixed depth rather than exhaustively checking all reachable states~\cite{biere1999symbolic}. While BMC cannot prove that properties hold universally, it can efficiently find bugs that manifest within the bound and provides coverage guarantees up to the specified depth. For smart contracts, where most vulnerabilities manifest within a small number of transactions, bounded model checking provides practical value.

\subsection{Theorem Proving}
\label{subsec:theorem-proving}

Theorem proving represents the most powerful—and most demanding—approach to formal verification. Rather than exhaustively exploring state space, theorem proving directly constructs mathematical proofs that programs satisfy specifications. Interactive theorem provers such as Coq~\cite{bertot2013interactive}, Isabelle~\cite{nipkow2002isabelle}, and Lean~\cite{moura2021lean} provide frameworks for developing machine-checked proofs with human guidance.

The expressiveness of theorem proving is unmatched: any mathematically statable property can, in principle, be proven. This includes properties beyond the reach of model checking and symbolic execution, such as correctness relative to complex mathematical specifications or security properties involving cryptographic assumptions. Projects such as the Ethereum Foundation's formal specification efforts demonstrate the application of theorem proving to blockchain protocol verification~\cite{hirai2017defining}.

However, the cost of theorem proving is substantial. Proofs require significant human expertise to construct, often taking person-months of effort for non-trivial systems. The proof development process requires not just programming skill but mathematical sophistication and familiarity with the proof assistant's logic and tactics. For most smart contract development contexts, this cost-benefit trade-off renders full theorem proving impractical.

Nonetheless, theorem proving plays an important role in the broader smart contract security ecosystem. Proofs of foundational libraries, such as safe arithmetic operations or standard token implementations, can be developed once and reused widely. High-value contracts managing billions of dollars may justify the investment in formal proofs. The verification of compiler correctness ensures that properties proven at the source level are preserved through compilation~\cite{leroy2009formal}.


\section{Summary and Transition to LLM-based Analysis}
\label{sec:summary-transition}

This chapter has surveyed the foundational techniques for program analysis as applied to smart contract security. Static analysis, operating on representations from abstract syntax trees to interprocedural data flow, provides scalable vulnerability detection with soundness guarantees but may produce false positives. Dynamic analysis, through transaction tracing and fuzz testing, observes actual behaviours with precision but cannot guarantee absence of vulnerabilities. Formal verification offers mathematical certainty but at significant computational and human cost.

Each technique occupies a distinct point in the trade-off space between soundness, completeness, scalability, and automation. In practice, effective security analysis combines multiple techniques: static analysis for broad coverage and early detection, dynamic analysis for validation and edge case discovery, and selective formal verification for highest-assurance components. This defence-in-depth approach acknowledges that no single technique suffices for comprehensive security assessment.

Despite the sophistication of these techniques, significant challenges remain. The semantic gap between source code and natural language makes it difficult for automated tools to understand programmer intent, leading to both false positives (flagging intended behaviour as bugs) and false negatives (missing bugs that violate unstated invariants). The evolving landscape of smart contract patterns, from novel DeFi primitives to cross-chain bridges, continuously introduces new vulnerability classes that existing detectors may not recognise~\cite{werner2022sok}.

Recent advances in large language models (LLMs) offer a promising complementary approach. LLMs trained on vast corpora of code and natural language have demonstrated remarkable capabilities in code understanding, generation, and analysis~\cite{chen2021evaluating}. Their ability to process natural language specifications, understand contextual patterns, and generalise from examples positions them as potential contributors to security analysis workflows.

However, LLMs also exhibit significant limitations for security-critical applications. They may hallucinate non-existent vulnerabilities, miss subtle bugs, or provide confident but incorrect assessments~\cite{pearce2022asleep}. Their probabilistic nature provides no formal guarantees, and their sensitivity to prompt formulation introduces variability in outputs. Integrating LLMs into security analysis requires careful consideration of their strengths and limitations.

The subsequent chapters explore how LLM capabilities can augment traditional analysis techniques. We examine retrieval-augmented generation (RAG) architectures that ground LLM outputs in verified knowledge bases, reducing hallucination while preserving the flexibility and natural language understanding that LLMs provide. Wake-AI, our proposed system, demonstrates this integration, combining Wake's precise static analysis with LLM-based contextual understanding and explanation generation. This hybrid approach seeks to leverage the complementary strengths of symbolic and neural methods, advancing toward more effective, accessible, and reliable smart contract security analysis.


\chapter{Large Language Models}

The detection of vulnerabilities in smart contracts has traditionally relied on rule-based static analysis tools and formal verification methods. While these approaches have proven effective for certain classes of vulnerabilities, they struggle with complex logic bugs and require extensive manual effort to develop detection rules \cite{luu2016making, tsankov2018securify}. Recent advances in deep learning, particularly the emergence of transformer-based Large Language Models (LLMs), have opened new possibilities for automated vulnerability detection that can learn patterns from code rather than relying solely on pre-defined rules \cite{chen2021evaluating, wang2024software}.

This chapter explores the architectural foundations of modern LLMs and their application to vulnerability detection in smart contracts. We begin by tracing the evolution from early neural network approaches to the transformer architecture that underlies contemporary LLMs. Understanding these foundations is essential for our later discussion of how Wake-AI leverages LLM capabilities and why techniques like Retrieval-Augmented Generation (RAG) and Model Context Protocol (MCP) become necessary when working with the constraints of real-world code analysis tasks.

\section{Transformers}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/transformer.png}
    \caption{Transformer architecture (illustrative).}
    \label{fig:transformer}
\end{figure}
\textit{TODO IMAGE: Replace Figure~\ref{fig:transformer} with an original diagram, or add a verified citation for the current image source.}

Before diving into transformer architecture, it's worth briefly reviewing the path that led to their development. Early attempts to apply machine learning to code analysis relied on simpler models such as n-gram language models and basic recurrent neural networks \cite{hindle2012naturalness}. These approaches treated code as a sequence of tokens and attempted to learn statistical patterns that could distinguish vulnerable from secure code.

Recurrent Neural Networks (RNNs) and their more sophisticated variants, Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long}, represented a significant step forward. By maintaining hidden states that captured information about previously seen tokens, LSTMs could theoretically model long-range dependencies in code. However, they suffered from fundamental limitations. The sequential nature of RNN processing meant that information had to be passed through many intermediate states to connect distant parts of a program, leading to the vanishing gradient problem during training \cite{bengio1994learning}. More critically, the sequential processing constraint prevented parallelization, making it impractical to train on the massive code corpora needed for general-purpose code understanding.

The transformer architecture, introduced by Vaswani et al. in their seminal 2017 paper "Attention Is All You Need" \cite{vaswani2017attention}, fundamentally changed this landscape. By replacing sequential recurrence with attention mechanisms that could directly relate any two positions in a sequence, transformers eliminated the sequential bottleneck while providing more direct paths for gradient flow during training. This architectural shift enabled both the massive scale of modern LLMs and their superior ability to capture complex relationships in code.

\section{Transformer Architecture}

The transformer architecture consists of an encoder-decoder structure, though many modern LLMs use only the decoder portion (GPT-family models) or only the encoder portion (BERT-family models) \cite{radford2019language, devlin2019bert}. Figure~\ref{fig:transformer} illustrates the complete transformer architecture. For vulnerability detection tasks, we primarily work with decoder-only models that have been pre-trained on large code corpora and can generate natural language explanations of detected issues.

At its core, a transformer processes sequences through a series of identical layers, each containing two main components: a multi-head self-attention mechanism and a position-wise feed-forward network. The architecture also employs residual connections around each of these sub-layers, followed by layer normalization \cite{ba2016layer}.

The input to the transformer begins with token embeddings that represent each element of the input sequence as a dense vector. For code analysis, these tokens might represent keywords, identifiers, operators, and other syntactic elements of the programming language. Crucially, since the attention mechanism itself has no inherent notion of sequence order, positional encodings are added to these embeddings to inject information about token positions.

\subsection{Self-Attention Mechanism}

The self-attention mechanism is the defining innovation of transformer architecture. Unlike RNNs that process tokens sequentially, self-attention allows the model to directly compute relationships between all pairs of tokens in parallel. This capability is particularly valuable for code analysis, where understanding a vulnerability often requires relating elements that are far apart in the token sequence—for example, connecting a variable declaration to its use in a security-critical operation several functions away.

The attention mechanism operates through three learned linear projections of the input: queries (Q), keys (K), and values (V). For each position in the sequence, the model computes attention scores that determine how much focus to place on every other position. Mathematically, the attention operation for a single head is defined as:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $d_k$ is the dimension of the key vectors \cite{vaswani2017attention}. The scaling factor $\sqrt{d_k}$ prevents the dot products from growing too large in magnitude, which would push the softmax function into regions with extremely small gradients.

To understand this intuitively, consider analyzing a function call in Solidity code. The query represents "what am I looking for" from the perspective of each token. The keys represent "what information do I contain" from each token's perspective. The dot product $QK^T$ computes compatibility scores between queries and keys—essentially asking "how relevant is each token to each other token?" The softmax normalizes these scores into a probability distribution, and finally, this distribution is used to take a weighted average of the values, producing an attention-weighted representation.

Multi-head attention extends this by running several attention operations in parallel, each with different learned projection matrices. The outputs are concatenated and linearly transformed:

\begin{equation}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}

where each $\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$ \cite{vaswani2017attention}. This multi-head design allows the model to attend to information from different representation subspaces simultaneously.

For vulnerability detection, this multi-head attention proves particularly powerful. One attention head might learn to track variable definitions and uses, another might focus on function call chains, and yet another might specialize in recognizing common vulnerability patterns. The model can then combine these different perspectives to make more nuanced judgments about potential security issues.

The transformer architecture includes two variants of attention: in the encoder, self-attention allows each token to attend to all tokens in the input sequence. In the decoder, masked self-attention restricts each token to only attend to previous tokens (and itself), which is essential for autoregressive generation tasks. When we use a pre-trained LLM for vulnerability analysis, we're typically leveraging this masked attention structure—the model has learned to predict subsequent tokens given previous context, and this predictive capability extends to understanding what code patterns might indicate vulnerabilities.

\subsection{Positional Encoding and Context Windows}

A fundamental characteristic of the attention mechanism is its permutation invariance: if we shuffled the input tokens, the attention computation would produce the same results (just in shuffled order). For natural language and code, where token order carries critical semantic information, this is clearly problematic. Positional encoding solves this by adding position-dependent signals to the token embeddings before they enter the transformer layers.

The original transformer used sinusoidal positional encodings \cite{vaswani2017attention}:

\begin{equation}
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{equation}
\begin{equation}
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{equation}

where $pos$ is the position in the sequence, $i$ is the dimension index, and $d_{model}$ is the model dimension. This encoding has useful mathematical properties: any fixed offset in position corresponds to a linear transformation in the encoding space, potentially allowing the model to learn to attend by relative positions. Moreover, these functions can extend to sequence lengths beyond those seen during training.

However, many modern LLMs use learned positional embeddings instead \cite{devlin2019bert, radford2019language}. While these must be explicitly trained for each position up to a maximum sequence length, they offer more flexibility and often perform slightly better in practice. More recent approaches like Rotary Position Embedding (RoPE) \cite{su2021roformer} and ALiBi \cite{press2022train} have shown promise in extending effective context lengths beyond training limits, an important consideration for analyzing long smart contracts.

The concept of a \textit{context window} refers to the maximum number of tokens a model can process at once. This is a hard constraint imposed by both the positional encoding scheme and computational resources—attention has $O(n^2)$ complexity with sequence length $n$, making it expensive for very long sequences. Early transformer models worked with context windows of 512 or 1024 tokens \cite{vaswani2017attention, devlin2019bert}. In deployed systems, available context windows vary by provider and model family; recent generations support substantially larger windows (hundreds of thousands of tokens and beyond for some models), which enables analysis over longer documents and larger codebases but does not remove the need for careful context selection \cite{openai_gpt41,anthropic_context_windows}.

For smart contract analysis, context window limitations pose a significant practical challenge. A typical smart contract might contain 500-2000 lines of Solidity code, which can translate to 5,000-20,000 tokens once we include relevant context like imported libraries, interface definitions, and documentation. More complex projects might exceed even generous context limits. This constraint motivates the need for Retrieval-Augmented Generation (RAG) approaches, which we explore in the next chapter. Rather than feeding an entire codebase into the model at once, RAG systems selectively retrieve the most relevant code segments, allowing the LLM to focus its limited context window on the information most likely to contain or explain vulnerabilities.

The feed-forward networks in each transformer layer also play a crucial role, though they're often overshadowed by the attention mechanism in discussions. After attention aggregates information across the sequence, the feed-forward network applies the same learned transformation to each position independently:

\begin{equation}
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\end{equation}

This two-layer network with ReLU activation significantly expands the model's capacity to learn complex patterns \cite{vaswani2017attention}. Recent work suggests these feed-forward layers serve as "key-value memories" where the model stores factual knowledge learned during pre-training \cite{geva2021transformer}.

Understanding these architectural components is essential for effectively applying LLMs to vulnerability detection. The attention mechanism's ability to capture long-range dependencies helps identify vulnerabilities that involve interactions between distant parts of code. The multi-head design allows specialization for different types of patterns. And the context window constraints explain why we need sophisticated retrieval strategies to handle real-world smart contracts. In the following chapters, we examine how these capabilities and limitations shape our approach to building Wake-AI, a system that combines static analysis tools like Wake with the pattern-recognition abilities of modern LLMs.


\section{Training paradigms}
\subsection{Pre-training}
\label{subsec:llm-pretraining}
Most LLMs are first \emph{pre-trained} on large corpora using self-supervised objectives that require no manual labeling. In decoder-only models, the dominant objective is next-token prediction: given a prefix, the model learns to predict the distribution of the next token. In encoder-only models such as BERT, masked language modeling instead trains the model to recover randomly masked tokens from surrounding context \cite{devlin2019bert}. Although the objectives differ, both encourage the model to build internal representations that capture statistical regularities in language and code.

For code, pre-training has two important implications. First, it teaches models syntax and idioms that are shared across many repositories, which helps with code comprehension and generation. Second, it also absorbs common-but-not-necessarily-correct patterns from open-source code, including insecure practices. This matters in security contexts: an LLM's familiarity with typical code can help it spot deviations or suspicious constructs, but pre-training alone does not provide guarantees that its conclusions are correct.
\subsection{Fine-tuning}
\label{subsec:llm-finetuning}
After pre-training, models are often adapted to downstream tasks via \emph{fine-tuning}. A common approach is supervised fine-tuning on instruction-response pairs so the model follows user intent more reliably. Many production models also incorporate alignment techniques such as reinforcement learning from human feedback (RLHF), which optimizes model outputs toward human preference judgments \cite{ouyang2022training}.

In security auditing, fine-tuning can improve usefulness in two ways. It can bias the model toward structured reporting (clear findings, evidence, remediation steps), and it can improve adherence to constraints (e.g., ``only claim what you can justify''). However, fine-tuning also risks overfitting to narrow datasets or encoding dataset-specific biases. For this thesis, we therefore treat fine-tuning as complementary rather than required: grounding and tool integration can improve reliability even when using general-purpose models.
\subsection{Prompting}
\label{subsec:llm-prompting}
\emph{Prompting} refers to shaping model behavior by providing instructions, context, and examples at inference time rather than changing model weights. Few-shot prompting, popularized by large-scale decoder-only models, demonstrates that including a small number of input-output examples can elicit task behavior without explicit fine-tuning \cite{brown2020language}. For code auditing, prompts often include: (i) a description of the vulnerability class to look for, (ii) the relevant code snippets, and (iii) an expected output format such as a checklist or a structured report.

Prompting is powerful but fragile. Small changes in wording can lead to different conclusions, and prompts that include large amounts of irrelevant context can degrade output quality. These realities motivate the later design of retrieval-augmented prompting: instead of relying on a single large prompt, we programmatically retrieve and assemble focused evidence that the model can cite and reason over.

\section{Code-Specialized Models and Embeddings}
% Explain why code models/embeddings help retrieval + similarity search (ties directly to your KB).
\subsection{Encoder Models for Code (e.g., CodeBERT)}
\label{subsec:code-encoders}
Encoder-only transformers can be trained to produce representations that are useful for classification and retrieval. CodeBERT is a representative example: it adapts the BERT-style masked language modeling objective to source code and natural language, producing embeddings that capture lexical and structural regularities in code \cite{feng2020codebert}. In security applications, such encoders are useful when the goal is to \emph{match} or \emph{rank} snippets (e.g., ``find similar vulnerability examples''), rather than to generate long-form explanations.

Compared with decoder-only LLMs, encoder models are typically cheaper to run and can embed large collections efficiently, which makes them a natural fit for building searchable knowledge bases. Their main limitation is that they do not directly produce free-form explanations, so they are usually paired with a generative model in a two-stage pipeline (retrieve with embeddings, then explain with an LLM).
\subsection{Seq2seq Code Models (T5/CodeT5 family)}
\label{subsec:code-seq2seq}
Sequence-to-sequence models map an input sequence to an output sequence and are therefore well-suited for code transformation tasks such as summarization, translation between languages, or automated patch generation. The CodeT5 family adapts the T5 framework to code by training on a mixture of code-related tasks and objectives, enabling both understanding-oriented and generation-oriented use cases \cite{wang2021codet5}. In the auditing setting, seq2seq models can help generate summaries of long functions or produce candidate fixes, but they still require careful validation because generated patches may be plausible yet incorrect.
\subsection{Code Embeddings for Retrieval}
\label{subsec:code-embeddings}
Retrieval systems typically use embeddings to map queries and documents into a shared vector space where semantic similarity corresponds to geometric proximity. In modern dense retrieval, a bi-encoder embeds the query and each candidate chunk independently and uses vector similarity to select top-$k$ candidates \cite{karpukhin2020dense}. This design scales well: embeddings can be precomputed for the corpus, and retrieval is fast even for large knowledge bases.

For this thesis, the key observation is that vulnerability knowledge is highly repetitive in its \emph{shape} (similar patterns recur across projects) but diverse in its \emph{surface form} (variable names, token standards, proxy patterns, and protocol-specific invariants). Embedding-based retrieval helps bridge this gap by retrieving conceptually similar examples even when the code differs syntactically. The retrieved chunks then serve as grounded evidence in prompts, improving factuality and making the model's reasoning easier to inspect.
% Put nomic-embed-code here if you used it; otherwise keep model-agnostic and mention you pick one in Implementation.

\section{State of the Art: LLMs in Smart Contract Auditing}
% Focus on 4 buckets: prompt-only, agentic, fine-tuned, RAG-augmented.
\subsection{Prompt-Only and Agentic Auditing Pipelines}
Early applications of LLMs to smart contract auditing often rely on \emph{prompt-only} workflows: users paste a contract (or a subset of it) and ask the model to identify issues and propose fixes. Systematic evaluations suggest that such approaches can surface real issues, but their reliability varies across vulnerability classes and depends strongly on prompt design and code size \cite{wu2025chatgpt}. These limitations become more pronounced for multi-contract systems and logic bugs that require cross-file reasoning.

Agentic pipelines address the context and workflow limitations by allowing the model to call tools and iterate. For example, an agent might first summarize each contract, then request specific functions, then run a static analyzer, and finally compose a report. The important shift is not the model itself but the \emph{orchestration}: tool calls provide structured, verifiable evidence and reduce the amount of raw code that must be placed into the model context. Wake-AI follows this general direction by embedding tool use into the audit workflow through structured interfaces.
\subsection{Fine-tuned Models for Smart Contract Vulnerability Detection}
Another line of work treats vulnerability detection as a supervised learning problem and fine-tunes models on labeled datasets of vulnerable contracts. These approaches can be effective at detecting known vulnerability patterns represented in the training distribution, especially when combined with careful dataset construction and evaluation protocols \cite{wang2024software}. However, they face two practical challenges. First, labeled smart-contract vulnerability datasets are relatively small compared to general code corpora, which can limit generalization. Second, real-world audits require explanations and evidence, not only classifications; a binary label is rarely sufficient to support remediation.

In practice, fine-tuned detectors are most useful when they are integrated into a broader workflow: they can triage and prioritize suspicious areas, while subsequent stages (manual review, static analysis, or LLM explanation grounded in retrieved evidence) provide the actionable details.
\subsection{RAG-Augmented Detection Frameworks}
Retrieval-Augmented Generation (RAG) addresses two recurring problems in prompt-only auditing: limited context windows and hallucination. Instead of asking a model to answer from parametric memory alone, a RAG system retrieves relevant external documents and conditions generation on that evidence \cite{lewis2020retrieval}. For smart contract auditing, the retrieved evidence can include vulnerability pattern descriptions, exploit write-ups, audit report excerpts, or internal project documentation.

Recent smart-contract-focused systems explore this idea explicitly. For example, EVuLLM proposes retrieving vulnerability knowledge and combining it with model reasoning to improve detection quality \cite{gao2024evullm}. SmartGuard similarly combines retrieval with a guardrail pipeline to improve auditing performance \cite{shen2024smartguard}. These designs motivate the approach taken in this thesis: build a curated knowledge base and integrate retrieval into an analysis framework (Wake) so that the LLM is consistently grounded in relevant, inspectable evidence.

\section{Prompting strategies}
\subsection{Task decomposition}
Complex auditing questions are rarely solved in one step. Task decomposition breaks an analysis into smaller sub-tasks such as ``summarize the contract's assets and trust boundaries'', ``identify external calls and state updates'', and ``check whether updates happen before interactions''. Decomposition improves reliability because each sub-task has narrower scope and clearer success criteria, and it naturally aligns with tool usage: static analysis can enumerate call sites, while the LLM focuses on explaining why a pattern is risky and what remediation looks like.
\subsection{Chain-of-thought}
% Describe conceptually; do not dump long CoT traces. Cite original Transformer-based reasoning work if needed.
Chain-of-thought prompting elicits intermediate reasoning steps and can improve performance on multi-step tasks by encouraging the model to keep track of assumptions and partial conclusions \cite{wei2022chain}. In security settings, however, the goal is not to obtain long informal traces, but to obtain \emph{checkable} reasoning: claims should be tied to specific code locations or retrieved evidence. In later chapters, we therefore structure outputs around findings, evidence, and remediation rather than relying on free-form reasoning traces.
\subsection{Few-shot learning}
Few-shot examples can calibrate what ``good'' output looks like. In auditing, examples are most valuable when they mirror the desired report structure (severity, impact, evidence, remediation) and when they demonstrate the level of conservatism expected: the model should prefer ``uncertain, needs evidence'' over confident speculation. This style is compatible with retrieval: retrieved examples become dynamic few-shot demonstrations that are tailored to the code under analysis rather than static prompt templates.

\section{Challenges and Limitations}
\subsection{Hallucinations and False Positives}
LLMs can produce plausible-sounding but incorrect statements, including claims about vulnerabilities that do not exist in the provided code. This is particularly problematic in auditing, where false positives waste reviewer time and may lead to unnecessary code changes. Empirical studies of code models report non-trivial rates of insecure or incorrect suggestions, especially when prompts are underspecified or when the model lacks relevant context \cite{pearce2022asleep}. A practical mitigation is to require evidence: every finding should reference concrete code behavior, tool outputs, or retrieved documentation rather than relying on model intuition alone.
\subsection{Context Window and Information Freshness}
Even with large context windows, real smart contract systems can exceed what is reasonable to include in a single prompt, especially when imported libraries, interfaces, and deployment configuration matter. Moreover, models have a fixed training cutoff; new vulnerability classes and best practices emerge continuously. RAG partially addresses both issues by retrieving up-to-date, task-relevant information at inference time and by focusing the prompt on a small set of high-signal chunks \cite{lewis2020retrieval}. The remaining challenge is selection: retrieval must be precise enough to avoid overwhelming the model with irrelevant text.
\subsection{Trust, Reproducibility, and Secure Tool Use}
% This is where you justify grounding with RAG + structured tool calls (MCP).
Audits are security-critical, so outputs must be reproducible and traceable. Purely generative answers are difficult to verify after the fact, and different runs may yield different results. Tool-augmented approaches improve reproducibility by logging which code artifacts and documents were used, and by separating verifiable computations (e.g., static analysis queries) from probabilistic summarization.

When agents can call external tools, additional safety concerns arise, such as prompt injection in retrieved documents and unintended execution of dangerous actions. For our setting, we mitigate these risks by restricting the agent's tool surface to read-only analysis and retrieval primitives and by treating all retrieved text as untrusted input that must not override analysis policies. [CITATION NEEDED]

\section{Summary and Transition to Wake}
LLMs offer a complementary capability to traditional program analysis: they can translate code behavior into natural language, relate patterns to prior examples, and assist in drafting remediation guidance. However, these strengths come with constraints that are especially visible in smart contract auditing: limited context windows, non-deterministic behavior, and the risk of hallucinated findings.

The practical takeaway is that LLMs are most useful when grounded in evidence and integrated into an analysis workflow. Retrieval provides focused, inspectable context, and structured tool calls provide precise program facts that the model can cite. The next chapter introduces the Wake framework, which supplies the program-analysis backbone for our system, and motivates how Wake-AI can serve as the workflow scaffold on which grounded LLM assistance is built.

\chapter{The Wake Framework}
% Goal: describe the platform you extend, and the baseline you improve.
\section{Overview}

\section{Internal Model}
% Keep aligned with hookup points for your implementation (IR/CFG/DDG/ICFG).
\subsection{Intermediate representation}
\subsection{Control flow graph}
\subsubsection*{Inter-procedural control flow graph}
\subsection{Data dependency graph}

\section{Analysis Infrastructure in Wake}
\subsection{Static Analysis and Detectors}
\subsection{Dynamic Analysis and Fuzz Testing Support}
\subsection{Auxiliary Tools (Printers, Debugging, Reporting)}

\section{Wake-AI}
% Cite wake-ai repo if that’s what you extend. :contentReference[oaicite:18]{index=18}
\subsection{Architecture}
\subsubsection*{Workflow steps}
\subsection{Prompt-Based Reasoning and Validation}
\subsection{Limitations}
% The key: “prompt-only, no grounded KB, context cost, inconsistent coverage.”

\section{Summary and Transition to Proposed Methodology}
% 5–8 lines: Wake-AI gives structure; your thesis adds grounding + retrieval via MCP.

\chapter{Proposed Methodology}
% THIS is your main “design contribution” chapter.
\section{Design Goals}
% Goals: groundedness, audit usefulness, token efficiency, modularity, reproducibility.
\section{System Architecture Overview}
% Diagram: Wake-AI steps + retrieval module + KB + MCP server + LLM.
% Cite MCP intro/spec. :contentReference[oaicite:19]{index=19}
\subsection{High-Level Workflow}

\section{Knowledge Base Design}
% What you store and why (vuln patterns, fixes, audit snippets); tie to OWASP/SWC.
\subsection{Vulnerability Pattern Database (Schema)}
% Use OWASP/SWC identifiers as primary keys. :contentReference[oaicite:20]{index=20}
\subsection{Audit Knowledge and Examples Database}
% Curate short “evidence” chunks, not huge reports (supports token efficiency).
\subsection{Indexing and Embeddings}
% How documents are chunked + embedded for retrieval.

\section{Retrieval augmented generation systen}
% Cite RAG paper for concept framing. :contentReference[oaicite:21]{index=21}
\subsection{Query Generation}
% How you create retrieval queries: from detectors, IR features, function summaries, etc.
\subsection{Re-ranking}
% How you select top-k, de-duplicate, and compress into prompt context.

\section{Model context protocol integration}
\subsection{Endpoints and Contracts}
% Define each endpoint (search patterns, fetch details, get examples, etc.) with expected inputs/outputs.
\subsubsection*{Security considerations}
% Timeouts, empty results, injection resistance, deterministic logging.

\section{Prompt engineering}
\subsection{TODO}

\section{Chapter Summary and Transition to Implementation}
% 5–8 lines: design choices -> now show how it’s implemented inside Wake-AI.

\chapter{Implementation}
% Goal: engineering details; keep it reproducible.
\section{Implementation Overview}
% Repo structure, modules added/modified, runtime flow.
\section{Knowledge Base Construction Pipeline}
% Data ingestion, cleaning, chunking, embedding, indexing, update strategy.
\section{MCP Server Implementation}
% Endpoints, request/response schema, retrieval backend, caching.
\section{Wake-AI Extension}
% Where you hook retrieval into steps; how context is stored/passed.
\section{Prompt Templates and Context Budgeting}
% Show how you cap tokens and prioritize evidence.
\section{End-to-End Example Walkthrough}
% One concise audit run demonstrating retrieval helping reasoning.

\section{Summary and Transition to Evaluation}
% 5–8 lines: implemented system exists -> now measure whether it helps vs baseline.

\chapter{Evaluation}
% Goal: show it works and understand when/why.
\section{Evaluation Questions}
% RQ1: Does RAG reduce hallucinations/false positives? RQ2: Does it improve coverage? RQ3: Cost/latency tradeoffs?
\section{Experimental Setup}
\subsection{Benchmarks and Datasets}
% Real vulnerable contracts + (optional) synthetic cases to test each pipeline component.
\subsection{Baselines}
% Wake-AI (prompt-only), Wake detectors, and Slither as external baseline. :contentReference[oaicite:22]{index=22}
\subsection{Metrics}
% Precision/recall per vulnerability class; “evidence correctness”; runtime/token cost.

\section{Experiments}
\subsection{Retrieval Quality (Component Test)}
% Top-k hit rate / qualitative relevance checks of retrieved patterns.
\subsection{End-to-End Vulnerability Detection}
% Compare baseline vs RAG-augmented on same contracts.
\subsection{Ablation Study}
% Remove reranking / remove audit KB / remove detector-seeded queries to show what matters.
\subsection{Qualitative Case Studies and Error Analysis}
% 2–3 focused cases: success, failure, “helped explanation but not detection,” etc.

\section{Threats to Validity}
% Dataset bias, label quality, prompt sensitivity, reproducibility, and generalization.
\section{Summary and Transition to Conclusion}
% 5–8 lines: what improved, what didn’t, and why.

\chapter{Conclusion}
\section{Summary of Contributions}
% One tight list: RAG+MCP integration, KB design, Wake-AI extension, empirical findings.
\section{Limitations}
% Where it fails: KB gaps, retrieval noise, complex protocol logic, environment assumptions.
\section{Future Work}
% Strong future directions: tighter static/IR-guided retrieval, symbolic checks, richer audit KB, standardized benchmarks.
